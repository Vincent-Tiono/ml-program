{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n44rMWcLS-BY"
   },
   "source": [
    "# Week 10: Colab Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCMvaDkMTJkT"
   },
   "source": [
    "# I. Introduction\n",
    "In this exercise, we apply CNN to MNIST data to classify the hand written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2jxq00nbuCwt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdXkw_9pkfn5"
   },
   "source": [
    "# Data Loading\n",
    "This code configures a PyTorch DataLoader object for the MNIST training and test datasets, with specified batch sizes and data transformations to provide data in a format that can be used efficiently for training and evaluating model machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PoUAesyDuL0n"
   },
   "outputs": [],
   "source": [
    "# Run this once to load the train and test data straight into a dataloader class\n",
    "# that will provide the batches\n",
    "\n",
    "# Set the batch sizes for the training and test datasets\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Creates a PyTorch DataLoader object for the training dataset,\n",
    "# this will be used to feed data to the model during training.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# Creates a PyTorch DataLoader object for the test dataset\n",
    "# This will be used to evaluate the model's performance.\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRXOY0Tzkfn6"
   },
   "source": [
    "# Visualize dataset sample\n",
    "This code configures a PyTorch DataLoader object for the MNIST training and test datasets, with specified batch sizes and data transformations to provide data in a format that can be used efficiently for training and evaluating model machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "boEAxlB5uPZx",
    "outputId": "98c422e3-aaff-47f6-cbed-98c10236724e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's draw some of the training data\n",
    "\n",
    "# Get an enumerator for the test_loader\n",
    "examples = enumerate(test_loader)\n",
    "\n",
    "# Get the next batch of examples from the enumerator\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "# Create a new figure for plotting\n",
    "fig = plt.figure()\n",
    "\n",
    "# Loop through the first 6 examples in the batch\n",
    "for i in range(6):\n",
    "    # Create a subplot in a 2x3 grid at position i+1\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "\n",
    "    # Adjust the layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the image data in grayscale with no interpolation\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "\n",
    "    # Set the title of the subplot to the ground truth label\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "\n",
    "    # Remove x-axis ticks\n",
    "    plt.xticks([])\n",
    "\n",
    "    # Remove y-axis ticks\n",
    "    plt.yticks([])\n",
    "\n",
    "# Show the figure with all subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_94InEd1TkC6"
   },
   "source": [
    "# II. Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y-BLHDvoPJh"
   },
   "source": [
    "1. The code defines two PyTorch neural network models, Net and Net2, with different architectures.\n",
    "\n",
    "2. It also includes a weights_init function that initializes the weights of the linear layers using Kaimin uniform distribution.\n",
    "\n",
    "3. The train and test functions are defined to train and evaluate the models on the MNIST dataset, and the code trains and tests both models, tracking their test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "y_kwOzXQuWNK"
   },
   "outputs": [],
   "source": [
    "from os import X_OK\n",
    "\n",
    "# This class implements a minimal network (which still does okay)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Valid convolution, 1 channel in, 2 channels out, stride 1, kernel size = 3\n",
    "        self.conv1 = nn.Conv2d(1, 2, kernel_size=3)\n",
    "        # Dropout for convolutions\n",
    "        self.drop = nn.Dropout2d()\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(338, 10)\n",
    "\n",
    "    # Define the forward pass of the network\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        # Apply dropout to the convolutional layer output\n",
    "        x = self.drop(x)\n",
    "        # Apply max pooling with a kernel size of 2\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Apply ReLU activation function\n",
    "        x = F.relu(x)\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.flatten(1)\n",
    "        # Apply the fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Apply log softmax to the output\n",
    "        x = F.log_softmax(x)\n",
    "        # Return the final output\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d4Ue45Pnf8gZ"
   },
   "outputs": [],
   "source": [
    "# Define a class Net2 that inherits from nn.Module\n",
    "class Net2(nn.Module):\n",
    "    # Initialize the layers of the neural network\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module\n",
    "        super(Net2, self).__init__()\n",
    "        # Define the first convolutional layer with 1 input channel, 10 output channels, and a kernel size of 5\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        # Define the second convolutional layer with 10 input channels, 20 output channels, and a kernel size of 5\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # Define a dropout layer for regularization\n",
    "        self.drop = nn.Dropout2d()\n",
    "        # Define the first fully connected layer with 320 input features and 50 output features\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        # Define the second fully connected layer with 50 input features and 10 output features\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    # Define the forward pass of the neural network\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolutional layer, followed by ReLU activation and max pooling\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # Apply the second convolutional layer, followed by dropout, ReLU activation, and max pooling\n",
    "        x = F.relu(F.max_pool2d(self.drop(self.conv2(x)), 2))\n",
    "        # Flatten the tensor into a 2D tensor with 320 features\n",
    "        x = x.view(-1, 320)\n",
    "        # Apply the first fully connected layer followed by ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Apply the second fully connected layer followed by log softmax activation\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        # Return the output tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9sN5hsK2uan8"
   },
   "outputs": [],
   "source": [
    "# He initialization of weights\n",
    "def weights_init(layer_in):\n",
    "    # Check if the layer is an instance of nn.Linear\n",
    "    if isinstance(layer_in, nn.Linear):\n",
    "        # Initialize the weights of the layer using Kaiming uniform distribution\n",
    "        nn.init.kaiming_uniform_(layer_in.weight)\n",
    "        # Fill the bias of the layer with zeros\n",
    "        layer_in.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2pBDgYp2ufUi"
   },
   "outputs": [],
   "source": [
    "# Main training routine\n",
    "\n",
    "# This code defines the main training routine,\n",
    "# where the model performs forward and backward passes on each batch,\n",
    "# updates parameters based on gradients,\n",
    "# and displays training progress every 10 batches.\n",
    "def train(epoch, model):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Iterate over batches of data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Zero the gradients of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Perform a forward pass through the model\n",
    "        output = model(data)\n",
    "        # Compute the negative log likelihood loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Perform a backward pass through the model\n",
    "        loss.backward()\n",
    "        # Update the model parameters based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training status every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            # Get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            # Count correct predictions\n",
    "            correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "             # Print training progress\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Xr6yXzWduhbU"
   },
   "outputs": [],
   "source": [
    "# Run on test data\n",
    "# This code defines a function that evaluates a machine learning model on a test dataset.\n",
    "# Then, it calculates the average loss and accuracy, and returns the test accuracy.\n",
    "\n",
    "# Define a function called test that takes an argument model to evaluate\n",
    "#the performance of the model on the test data.\n",
    "def test(model):\n",
    "\n",
    "  # Set the model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # initialize test_loss and correct,\n",
    "  # to keep track of the total loss and the number of\n",
    "  # correct predictions on the test dataset.\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "\n",
    "  # Disable gradient calculation to save memory\n",
    "  with torch.no_grad():\n",
    "\n",
    "    # Iterate over the test_loader\n",
    "    for data, target in test_loader:\n",
    "\n",
    "      # Pass the current batch of data through the model\n",
    "      # and store the output in the output variable\n",
    "      output = model(data)\n",
    "\n",
    "      # Calculate the negative log likelihood loss of the output in the test_loss variable\n",
    "      # size_average=False ensures that the loss is not averaged, but instead summed up.\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "\n",
    "      # Store the index of the maximum value of the output tensor in the pred variable\n",
    "      # max(1, keepdim=True) part of the code ensures that the output\n",
    "      # has the same number of dimensions as the target labels.\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "      # Compare the model's predictions (pred) with the true target labels (target),\n",
    "      # Count the number of correct predictions, accumulating the result in the correct variable.\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "  # Calculate the average test_loss\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "\n",
    "  # Print out the average test loss and the test accuracy as a percentage.\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "  # Return the test accuracy as a percentage\n",
    "  return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVUrbYiamki8",
    "outputId": "79438fd8-f140-4794-e6ce-24e55b7bad76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/hqqt4z8949bf6xhx7jfq4f_40000gn/T/ipykernel_6233/2053953840.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.4545, Accuracy: 1008/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.608022\n",
      "Train Epoch: 1 [640/60000]\tLoss: 2.397586\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 2.212079\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 2.185869\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 1.876151\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 1.915088\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 1.749066\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 1.581131\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 1.620273\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 1.562097\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 1.311533\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 1.442856\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 1.438558\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 1.494274\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 1.348970\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 1.352128\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 1.210430\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 1.365772\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 1.430260\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 1.085732\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 1.056072\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 1.284158\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 1.270782\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 1.042769\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 1.114892\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 1.045083\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 1.246925\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 1.115791\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 1.139500\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 1.020339\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 1.193469\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 1.206930\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 1.169970\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 1.127583\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 0.963250\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 0.819391\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 1.021007\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 0.817959\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 0.936117\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 1.277377\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 0.923660\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 0.788201\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 0.958768\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 0.836600\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 0.787386\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 0.922442\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 0.849889\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 0.983485\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 1.004523\n",
      "Train Epoch: 1 [31360/60000]\tLoss: 1.147570\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 1.080097\n",
      "Train Epoch: 1 [32640/60000]\tLoss: 0.809954\n",
      "Train Epoch: 1 [33280/60000]\tLoss: 1.112917\n",
      "Train Epoch: 1 [33920/60000]\tLoss: 0.900204\n",
      "Train Epoch: 1 [34560/60000]\tLoss: 0.890056\n",
      "Train Epoch: 1 [35200/60000]\tLoss: 0.726399\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 0.850646\n",
      "Train Epoch: 1 [36480/60000]\tLoss: 1.046499\n",
      "Train Epoch: 1 [37120/60000]\tLoss: 0.762302\n",
      "Train Epoch: 1 [37760/60000]\tLoss: 0.757739\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 1.220247\n",
      "Train Epoch: 1 [39040/60000]\tLoss: 0.852908\n",
      "Train Epoch: 1 [39680/60000]\tLoss: 0.792475\n",
      "Train Epoch: 1 [40320/60000]\tLoss: 0.828624\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 0.864830\n",
      "Train Epoch: 1 [41600/60000]\tLoss: 0.898855\n",
      "Train Epoch: 1 [42240/60000]\tLoss: 0.872687\n",
      "Train Epoch: 1 [42880/60000]\tLoss: 0.990649\n",
      "Train Epoch: 1 [43520/60000]\tLoss: 0.938774\n",
      "Train Epoch: 1 [44160/60000]\tLoss: 1.091617\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 1.082691\n",
      "Train Epoch: 1 [45440/60000]\tLoss: 0.783838\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 1.198946\n",
      "Train Epoch: 1 [46720/60000]\tLoss: 1.056316\n",
      "Train Epoch: 1 [47360/60000]\tLoss: 1.055746\n",
      "Train Epoch: 1 [48000/60000]\tLoss: 1.041187\n",
      "Train Epoch: 1 [48640/60000]\tLoss: 1.179948\n",
      "Train Epoch: 1 [49280/60000]\tLoss: 0.949312\n",
      "Train Epoch: 1 [49920/60000]\tLoss: 1.110480\n",
      "Train Epoch: 1 [50560/60000]\tLoss: 1.051746\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 0.914902\n",
      "Train Epoch: 1 [51840/60000]\tLoss: 0.639399\n",
      "Train Epoch: 1 [52480/60000]\tLoss: 0.934905\n",
      "Train Epoch: 1 [53120/60000]\tLoss: 0.897879\n",
      "Train Epoch: 1 [53760/60000]\tLoss: 0.703993\n",
      "Train Epoch: 1 [54400/60000]\tLoss: 1.063131\n",
      "Train Epoch: 1 [55040/60000]\tLoss: 0.938784\n",
      "Train Epoch: 1 [55680/60000]\tLoss: 0.840122\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 0.712827\n",
      "Train Epoch: 1 [56960/60000]\tLoss: 0.844504\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 0.895887\n",
      "Train Epoch: 1 [58240/60000]\tLoss: 0.819361\n",
      "Train Epoch: 1 [58880/60000]\tLoss: 0.888985\n",
      "Train Epoch: 1 [59520/60000]\tLoss: 0.696064\n",
      "Train Epoch: 2 [0/60000]\tLoss: 1.388562\n",
      "Train Epoch: 2 [640/60000]\tLoss: 0.799029\n",
      "Train Epoch: 2 [1280/60000]\tLoss: 1.121222\n",
      "Train Epoch: 2 [1920/60000]\tLoss: 1.077744\n",
      "Train Epoch: 2 [2560/60000]\tLoss: 1.028595\n",
      "Train Epoch: 2 [3200/60000]\tLoss: 0.955840\n",
      "Train Epoch: 2 [3840/60000]\tLoss: 0.846616\n",
      "Train Epoch: 2 [4480/60000]\tLoss: 1.012987\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 0.814812\n",
      "Train Epoch: 2 [5760/60000]\tLoss: 0.860853\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 0.905192\n",
      "Train Epoch: 2 [7040/60000]\tLoss: 0.664836\n",
      "Train Epoch: 2 [7680/60000]\tLoss: 1.013190\n",
      "Train Epoch: 2 [8320/60000]\tLoss: 1.236112\n",
      "Train Epoch: 2 [8960/60000]\tLoss: 0.953024\n",
      "Train Epoch: 2 [9600/60000]\tLoss: 0.888109\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 0.958566\n",
      "Train Epoch: 2 [10880/60000]\tLoss: 1.234181\n",
      "Train Epoch: 2 [11520/60000]\tLoss: 0.711339\n",
      "Train Epoch: 2 [12160/60000]\tLoss: 0.727140\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 1.014783\n",
      "Train Epoch: 2 [13440/60000]\tLoss: 0.843751\n",
      "Train Epoch: 2 [14080/60000]\tLoss: 0.728734\n",
      "Train Epoch: 2 [14720/60000]\tLoss: 0.858825\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 0.847689\n",
      "Train Epoch: 2 [16000/60000]\tLoss: 0.879507\n",
      "Train Epoch: 2 [16640/60000]\tLoss: 0.738703\n",
      "Train Epoch: 2 [17280/60000]\tLoss: 1.086554\n",
      "Train Epoch: 2 [17920/60000]\tLoss: 0.797429\n",
      "Train Epoch: 2 [18560/60000]\tLoss: 0.919035\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.964503\n",
      "Train Epoch: 2 [19840/60000]\tLoss: 0.796528\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 0.835181\n",
      "Train Epoch: 2 [21120/60000]\tLoss: 0.803234\n",
      "Train Epoch: 2 [21760/60000]\tLoss: 0.554298\n",
      "Train Epoch: 2 [22400/60000]\tLoss: 0.697019\n",
      "Train Epoch: 2 [23040/60000]\tLoss: 0.859366\n",
      "Train Epoch: 2 [23680/60000]\tLoss: 1.045364\n",
      "Train Epoch: 2 [24320/60000]\tLoss: 1.401612\n",
      "Train Epoch: 2 [24960/60000]\tLoss: 1.095616\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.754969\n",
      "Train Epoch: 2 [26240/60000]\tLoss: 0.952720\n",
      "Train Epoch: 2 [26880/60000]\tLoss: 0.940290\n",
      "Train Epoch: 2 [27520/60000]\tLoss: 0.942545\n",
      "Train Epoch: 2 [28160/60000]\tLoss: 0.996916\n",
      "Train Epoch: 2 [28800/60000]\tLoss: 0.808082\n",
      "Train Epoch: 2 [29440/60000]\tLoss: 1.299026\n",
      "Train Epoch: 2 [30080/60000]\tLoss: 0.797366\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 0.898021\n",
      "Train Epoch: 2 [31360/60000]\tLoss: 0.664263\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.782407\n",
      "Train Epoch: 2 [32640/60000]\tLoss: 0.799176\n",
      "Train Epoch: 2 [33280/60000]\tLoss: 0.847445\n",
      "Train Epoch: 2 [33920/60000]\tLoss: 0.925189\n",
      "Train Epoch: 2 [34560/60000]\tLoss: 0.820335\n",
      "Train Epoch: 2 [35200/60000]\tLoss: 1.227395\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 0.760125\n",
      "Train Epoch: 2 [36480/60000]\tLoss: 0.833015\n",
      "Train Epoch: 2 [37120/60000]\tLoss: 0.829719\n",
      "Train Epoch: 2 [37760/60000]\tLoss: 0.906636\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 0.986617\n",
      "Train Epoch: 2 [39040/60000]\tLoss: 0.760421\n",
      "Train Epoch: 2 [39680/60000]\tLoss: 1.225787\n",
      "Train Epoch: 2 [40320/60000]\tLoss: 0.729583\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 0.838581\n",
      "Train Epoch: 2 [41600/60000]\tLoss: 0.726741\n",
      "Train Epoch: 2 [42240/60000]\tLoss: 0.986363\n",
      "Train Epoch: 2 [42880/60000]\tLoss: 1.050276\n",
      "Train Epoch: 2 [43520/60000]\tLoss: 0.777433\n",
      "Train Epoch: 2 [44160/60000]\tLoss: 0.669819\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 0.768627\n",
      "Train Epoch: 2 [45440/60000]\tLoss: 1.094334\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 1.142151\n",
      "Train Epoch: 2 [46720/60000]\tLoss: 0.757564\n",
      "Train Epoch: 2 [47360/60000]\tLoss: 0.890203\n",
      "Train Epoch: 2 [48000/60000]\tLoss: 0.778253\n",
      "Train Epoch: 2 [48640/60000]\tLoss: 0.924829\n",
      "Train Epoch: 2 [49280/60000]\tLoss: 0.829193\n",
      "Train Epoch: 2 [49920/60000]\tLoss: 1.098284\n",
      "Train Epoch: 2 [50560/60000]\tLoss: 0.750007\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.861953\n",
      "Train Epoch: 2 [51840/60000]\tLoss: 0.760925\n",
      "Train Epoch: 2 [52480/60000]\tLoss: 0.528441\n",
      "Train Epoch: 2 [53120/60000]\tLoss: 0.815614\n",
      "Train Epoch: 2 [53760/60000]\tLoss: 0.632734\n",
      "Train Epoch: 2 [54400/60000]\tLoss: 1.215880\n",
      "Train Epoch: 2 [55040/60000]\tLoss: 0.864890\n",
      "Train Epoch: 2 [55680/60000]\tLoss: 0.666157\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 0.841616\n",
      "Train Epoch: 2 [56960/60000]\tLoss: 0.863801\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 1.081041\n",
      "Train Epoch: 2 [58240/60000]\tLoss: 0.997978\n",
      "Train Epoch: 2 [58880/60000]\tLoss: 0.815919\n",
      "Train Epoch: 2 [59520/60000]\tLoss: 0.519941\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.968070\n",
      "Train Epoch: 3 [640/60000]\tLoss: 0.702319\n",
      "Train Epoch: 3 [1280/60000]\tLoss: 0.899907\n",
      "Train Epoch: 3 [1920/60000]\tLoss: 0.773986\n",
      "Train Epoch: 3 [2560/60000]\tLoss: 0.978498\n",
      "Train Epoch: 3 [3200/60000]\tLoss: 0.794129\n",
      "Train Epoch: 3 [3840/60000]\tLoss: 0.861290\n",
      "Train Epoch: 3 [4480/60000]\tLoss: 0.573036\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.977881\n",
      "Train Epoch: 3 [5760/60000]\tLoss: 0.943521\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 0.859114\n",
      "Train Epoch: 3 [7040/60000]\tLoss: 0.849834\n",
      "Train Epoch: 3 [7680/60000]\tLoss: 0.824449\n",
      "Train Epoch: 3 [8320/60000]\tLoss: 0.711804\n",
      "Train Epoch: 3 [8960/60000]\tLoss: 0.730485\n",
      "Train Epoch: 3 [9600/60000]\tLoss: 0.971618\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.614205\n",
      "Train Epoch: 3 [10880/60000]\tLoss: 0.643104\n",
      "Train Epoch: 3 [11520/60000]\tLoss: 0.562848\n",
      "Train Epoch: 3 [12160/60000]\tLoss: 0.690458\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.777421\n",
      "Train Epoch: 3 [13440/60000]\tLoss: 0.826884\n",
      "Train Epoch: 3 [14080/60000]\tLoss: 0.831686\n",
      "Train Epoch: 3 [14720/60000]\tLoss: 0.758439\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 0.828410\n",
      "Train Epoch: 3 [16000/60000]\tLoss: 0.736688\n",
      "Train Epoch: 3 [16640/60000]\tLoss: 0.992924\n",
      "Train Epoch: 3 [17280/60000]\tLoss: 0.679847\n",
      "Train Epoch: 3 [17920/60000]\tLoss: 0.844140\n",
      "Train Epoch: 3 [18560/60000]\tLoss: 0.650100\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 0.608705\n",
      "Train Epoch: 3 [19840/60000]\tLoss: 1.026376\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.746344\n",
      "Train Epoch: 3 [21120/60000]\tLoss: 0.769647\n",
      "Train Epoch: 3 [21760/60000]\tLoss: 0.773320\n",
      "Train Epoch: 3 [22400/60000]\tLoss: 0.682031\n",
      "Train Epoch: 3 [23040/60000]\tLoss: 1.079723\n",
      "Train Epoch: 3 [23680/60000]\tLoss: 0.665823\n",
      "Train Epoch: 3 [24320/60000]\tLoss: 0.840071\n",
      "Train Epoch: 3 [24960/60000]\tLoss: 0.887775\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.917497\n",
      "Train Epoch: 3 [26240/60000]\tLoss: 0.675210\n",
      "Train Epoch: 3 [26880/60000]\tLoss: 1.406154\n",
      "Train Epoch: 3 [27520/60000]\tLoss: 0.937310\n",
      "Train Epoch: 3 [28160/60000]\tLoss: 0.716656\n",
      "Train Epoch: 3 [28800/60000]\tLoss: 0.816284\n",
      "Train Epoch: 3 [29440/60000]\tLoss: 0.917742\n",
      "Train Epoch: 3 [30080/60000]\tLoss: 0.670767\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 1.140088\n",
      "Train Epoch: 3 [31360/60000]\tLoss: 1.020848\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 1.010141\n",
      "Train Epoch: 3 [32640/60000]\tLoss: 1.064349\n",
      "Train Epoch: 3 [33280/60000]\tLoss: 0.996039\n",
      "Train Epoch: 3 [33920/60000]\tLoss: 0.606262\n",
      "Train Epoch: 3 [34560/60000]\tLoss: 0.724928\n",
      "Train Epoch: 3 [35200/60000]\tLoss: 1.087496\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 0.573564\n",
      "Train Epoch: 3 [36480/60000]\tLoss: 0.940470\n",
      "Train Epoch: 3 [37120/60000]\tLoss: 0.868615\n",
      "Train Epoch: 3 [37760/60000]\tLoss: 0.886711\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.834097\n",
      "Train Epoch: 3 [39040/60000]\tLoss: 0.636147\n",
      "Train Epoch: 3 [39680/60000]\tLoss: 0.992011\n",
      "Train Epoch: 3 [40320/60000]\tLoss: 0.808503\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.614718\n",
      "Train Epoch: 3 [41600/60000]\tLoss: 1.001377\n",
      "Train Epoch: 3 [42240/60000]\tLoss: 0.638700\n",
      "Train Epoch: 3 [42880/60000]\tLoss: 0.863850\n",
      "Train Epoch: 3 [43520/60000]\tLoss: 1.059789\n",
      "Train Epoch: 3 [44160/60000]\tLoss: 0.750259\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 1.072254\n",
      "Train Epoch: 3 [45440/60000]\tLoss: 0.774994\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 0.773401\n",
      "Train Epoch: 3 [46720/60000]\tLoss: 0.780993\n",
      "Train Epoch: 3 [47360/60000]\tLoss: 0.873764\n",
      "Train Epoch: 3 [48000/60000]\tLoss: 0.829669\n",
      "Train Epoch: 3 [48640/60000]\tLoss: 0.699379\n",
      "Train Epoch: 3 [49280/60000]\tLoss: 1.278420\n",
      "Train Epoch: 3 [49920/60000]\tLoss: 0.810552\n",
      "Train Epoch: 3 [50560/60000]\tLoss: 0.811588\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.694779\n",
      "Train Epoch: 3 [51840/60000]\tLoss: 0.916545\n",
      "Train Epoch: 3 [52480/60000]\tLoss: 0.746704\n",
      "Train Epoch: 3 [53120/60000]\tLoss: 1.028772\n",
      "Train Epoch: 3 [53760/60000]\tLoss: 0.800316\n",
      "Train Epoch: 3 [54400/60000]\tLoss: 0.974020\n",
      "Train Epoch: 3 [55040/60000]\tLoss: 1.164435\n",
      "Train Epoch: 3 [55680/60000]\tLoss: 0.833985\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.753729\n",
      "Train Epoch: 3 [56960/60000]\tLoss: 0.871614\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.734176\n",
      "Train Epoch: 3 [58240/60000]\tLoss: 0.657572\n",
      "Train Epoch: 3 [58880/60000]\tLoss: 0.639578\n",
      "Train Epoch: 3 [59520/60000]\tLoss: 0.913873\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.594862\n",
      "Train Epoch: 4 [640/60000]\tLoss: 0.894203\n",
      "Train Epoch: 4 [1280/60000]\tLoss: 0.845474\n",
      "Train Epoch: 4 [1920/60000]\tLoss: 0.874652\n",
      "Train Epoch: 4 [2560/60000]\tLoss: 0.677884\n",
      "Train Epoch: 4 [3200/60000]\tLoss: 0.757078\n",
      "Train Epoch: 4 [3840/60000]\tLoss: 0.553463\n",
      "Train Epoch: 4 [4480/60000]\tLoss: 0.864110\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.625697\n",
      "Train Epoch: 4 [5760/60000]\tLoss: 0.742330\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 1.015085\n",
      "Train Epoch: 4 [7040/60000]\tLoss: 0.821548\n",
      "Train Epoch: 4 [7680/60000]\tLoss: 0.600248\n",
      "Train Epoch: 4 [8320/60000]\tLoss: 0.820621\n",
      "Train Epoch: 4 [8960/60000]\tLoss: 0.743465\n",
      "Train Epoch: 4 [9600/60000]\tLoss: 0.784192\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.921047\n",
      "Train Epoch: 4 [10880/60000]\tLoss: 1.195621\n",
      "Train Epoch: 4 [11520/60000]\tLoss: 0.615339\n",
      "Train Epoch: 4 [12160/60000]\tLoss: 0.936621\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.611666\n",
      "Train Epoch: 4 [13440/60000]\tLoss: 0.603366\n",
      "Train Epoch: 4 [14080/60000]\tLoss: 0.644805\n",
      "Train Epoch: 4 [14720/60000]\tLoss: 1.010561\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 0.819784\n",
      "Train Epoch: 4 [16000/60000]\tLoss: 0.881771\n",
      "Train Epoch: 4 [16640/60000]\tLoss: 0.498674\n",
      "Train Epoch: 4 [17280/60000]\tLoss: 0.768213\n",
      "Train Epoch: 4 [17920/60000]\tLoss: 0.849593\n",
      "Train Epoch: 4 [18560/60000]\tLoss: 0.947194\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.916384\n",
      "Train Epoch: 4 [19840/60000]\tLoss: 0.717944\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.897866\n",
      "Train Epoch: 4 [21120/60000]\tLoss: 1.022593\n",
      "Train Epoch: 4 [21760/60000]\tLoss: 0.757854\n",
      "Train Epoch: 4 [22400/60000]\tLoss: 0.774203\n",
      "Train Epoch: 4 [23040/60000]\tLoss: 0.937490\n",
      "Train Epoch: 4 [23680/60000]\tLoss: 0.822950\n",
      "Train Epoch: 4 [24320/60000]\tLoss: 0.799074\n",
      "Train Epoch: 4 [24960/60000]\tLoss: 0.762284\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.741589\n",
      "Train Epoch: 4 [26240/60000]\tLoss: 0.944756\n",
      "Train Epoch: 4 [26880/60000]\tLoss: 0.733877\n",
      "Train Epoch: 4 [27520/60000]\tLoss: 1.190324\n",
      "Train Epoch: 4 [28160/60000]\tLoss: 0.972272\n",
      "Train Epoch: 4 [28800/60000]\tLoss: 0.955248\n",
      "Train Epoch: 4 [29440/60000]\tLoss: 0.787616\n",
      "Train Epoch: 4 [30080/60000]\tLoss: 0.896278\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 0.972952\n",
      "Train Epoch: 4 [31360/60000]\tLoss: 0.792044\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.912175\n",
      "Train Epoch: 4 [32640/60000]\tLoss: 0.747722\n",
      "Train Epoch: 4 [33280/60000]\tLoss: 0.783206\n",
      "Train Epoch: 4 [33920/60000]\tLoss: 0.601630\n",
      "Train Epoch: 4 [34560/60000]\tLoss: 0.890565\n",
      "Train Epoch: 4 [35200/60000]\tLoss: 1.097994\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.867874\n",
      "Train Epoch: 4 [36480/60000]\tLoss: 0.879671\n",
      "Train Epoch: 4 [37120/60000]\tLoss: 0.785627\n",
      "Train Epoch: 4 [37760/60000]\tLoss: 0.839970\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 0.799894\n",
      "Train Epoch: 4 [39040/60000]\tLoss: 0.753602\n",
      "Train Epoch: 4 [39680/60000]\tLoss: 0.629616\n",
      "Train Epoch: 4 [40320/60000]\tLoss: 0.873152\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.567197\n",
      "Train Epoch: 4 [41600/60000]\tLoss: 0.922947\n",
      "Train Epoch: 4 [42240/60000]\tLoss: 0.850299\n",
      "Train Epoch: 4 [42880/60000]\tLoss: 0.930822\n",
      "Train Epoch: 4 [43520/60000]\tLoss: 1.122842\n",
      "Train Epoch: 4 [44160/60000]\tLoss: 1.114902\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 0.793211\n",
      "Train Epoch: 4 [45440/60000]\tLoss: 0.982882\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.982182\n",
      "Train Epoch: 4 [46720/60000]\tLoss: 0.715335\n",
      "Train Epoch: 4 [47360/60000]\tLoss: 0.649976\n",
      "Train Epoch: 4 [48000/60000]\tLoss: 0.810842\n",
      "Train Epoch: 4 [48640/60000]\tLoss: 0.886558\n",
      "Train Epoch: 4 [49280/60000]\tLoss: 0.743344\n",
      "Train Epoch: 4 [49920/60000]\tLoss: 0.884163\n",
      "Train Epoch: 4 [50560/60000]\tLoss: 0.959961\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 1.010227\n",
      "Train Epoch: 4 [51840/60000]\tLoss: 0.868522\n",
      "Train Epoch: 4 [52480/60000]\tLoss: 0.747014\n",
      "Train Epoch: 4 [53120/60000]\tLoss: 0.772648\n",
      "Train Epoch: 4 [53760/60000]\tLoss: 0.778040\n",
      "Train Epoch: 4 [54400/60000]\tLoss: 0.775949\n",
      "Train Epoch: 4 [55040/60000]\tLoss: 0.794433\n",
      "Train Epoch: 4 [55680/60000]\tLoss: 0.894713\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.710094\n",
      "Train Epoch: 4 [56960/60000]\tLoss: 0.720446\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.966007\n",
      "Train Epoch: 4 [58240/60000]\tLoss: 0.643893\n",
      "Train Epoch: 4 [58880/60000]\tLoss: 1.020676\n",
      "Train Epoch: 4 [59520/60000]\tLoss: 0.661775\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.832612\n",
      "Train Epoch: 5 [640/60000]\tLoss: 0.939221\n",
      "Train Epoch: 5 [1280/60000]\tLoss: 0.829882\n",
      "Train Epoch: 5 [1920/60000]\tLoss: 0.829436\n",
      "Train Epoch: 5 [2560/60000]\tLoss: 0.744923\n",
      "Train Epoch: 5 [3200/60000]\tLoss: 0.612148\n",
      "Train Epoch: 5 [3840/60000]\tLoss: 0.827836\n",
      "Train Epoch: 5 [4480/60000]\tLoss: 0.896769\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.907325\n",
      "Train Epoch: 5 [5760/60000]\tLoss: 0.772456\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.698228\n",
      "Train Epoch: 5 [7040/60000]\tLoss: 0.719434\n",
      "Train Epoch: 5 [7680/60000]\tLoss: 0.759107\n",
      "Train Epoch: 5 [8320/60000]\tLoss: 1.009980\n",
      "Train Epoch: 5 [8960/60000]\tLoss: 0.583051\n",
      "Train Epoch: 5 [9600/60000]\tLoss: 0.435073\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 1.008756\n",
      "Train Epoch: 5 [10880/60000]\tLoss: 0.670932\n",
      "Train Epoch: 5 [11520/60000]\tLoss: 0.800492\n",
      "Train Epoch: 5 [12160/60000]\tLoss: 0.895763\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 0.748824\n",
      "Train Epoch: 5 [13440/60000]\tLoss: 1.001313\n",
      "Train Epoch: 5 [14080/60000]\tLoss: 0.901018\n",
      "Train Epoch: 5 [14720/60000]\tLoss: 0.785754\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 0.821954\n",
      "Train Epoch: 5 [16000/60000]\tLoss: 0.807874\n",
      "Train Epoch: 5 [16640/60000]\tLoss: 0.844806\n",
      "Train Epoch: 5 [17280/60000]\tLoss: 0.785897\n",
      "Train Epoch: 5 [17920/60000]\tLoss: 0.832211\n",
      "Train Epoch: 5 [18560/60000]\tLoss: 0.698587\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.699868\n",
      "Train Epoch: 5 [19840/60000]\tLoss: 0.664219\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.956035\n",
      "Train Epoch: 5 [21120/60000]\tLoss: 0.887298\n",
      "Train Epoch: 5 [21760/60000]\tLoss: 0.732874\n",
      "Train Epoch: 5 [22400/60000]\tLoss: 0.950727\n",
      "Train Epoch: 5 [23040/60000]\tLoss: 0.853863\n",
      "Train Epoch: 5 [23680/60000]\tLoss: 0.852757\n",
      "Train Epoch: 5 [24320/60000]\tLoss: 0.929907\n",
      "Train Epoch: 5 [24960/60000]\tLoss: 0.936567\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.750889\n",
      "Train Epoch: 5 [26240/60000]\tLoss: 0.809088\n",
      "Train Epoch: 5 [26880/60000]\tLoss: 1.061948\n",
      "Train Epoch: 5 [27520/60000]\tLoss: 0.821784\n",
      "Train Epoch: 5 [28160/60000]\tLoss: 0.782960\n",
      "Train Epoch: 5 [28800/60000]\tLoss: 0.961394\n",
      "Train Epoch: 5 [29440/60000]\tLoss: 0.939022\n",
      "Train Epoch: 5 [30080/60000]\tLoss: 0.668311\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 0.747455\n",
      "Train Epoch: 5 [31360/60000]\tLoss: 0.860766\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.799657\n",
      "Train Epoch: 5 [32640/60000]\tLoss: 0.958175\n",
      "Train Epoch: 5 [33280/60000]\tLoss: 0.863894\n",
      "Train Epoch: 5 [33920/60000]\tLoss: 0.859970\n",
      "Train Epoch: 5 [34560/60000]\tLoss: 0.912315\n",
      "Train Epoch: 5 [35200/60000]\tLoss: 0.719610\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 0.880912\n",
      "Train Epoch: 5 [36480/60000]\tLoss: 0.470744\n",
      "Train Epoch: 5 [37120/60000]\tLoss: 0.962957\n",
      "Train Epoch: 5 [37760/60000]\tLoss: 0.958375\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 0.752234\n",
      "Train Epoch: 5 [39040/60000]\tLoss: 0.880981\n",
      "Train Epoch: 5 [39680/60000]\tLoss: 0.672553\n",
      "Train Epoch: 5 [40320/60000]\tLoss: 0.676392\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 1.040627\n",
      "Train Epoch: 5 [41600/60000]\tLoss: 0.818135\n",
      "Train Epoch: 5 [42240/60000]\tLoss: 0.860892\n",
      "Train Epoch: 5 [42880/60000]\tLoss: 0.696423\n",
      "Train Epoch: 5 [43520/60000]\tLoss: 0.811611\n",
      "Train Epoch: 5 [44160/60000]\tLoss: 0.575922\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 0.673576\n",
      "Train Epoch: 5 [45440/60000]\tLoss: 0.799796\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 0.706782\n",
      "Train Epoch: 5 [46720/60000]\tLoss: 0.617838\n",
      "Train Epoch: 5 [47360/60000]\tLoss: 0.895750\n",
      "Train Epoch: 5 [48000/60000]\tLoss: 0.948426\n",
      "Train Epoch: 5 [48640/60000]\tLoss: 0.957021\n",
      "Train Epoch: 5 [49280/60000]\tLoss: 0.741177\n",
      "Train Epoch: 5 [49920/60000]\tLoss: 0.589621\n",
      "Train Epoch: 5 [50560/60000]\tLoss: 0.715481\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.607738\n",
      "Train Epoch: 5 [51840/60000]\tLoss: 0.590120\n",
      "Train Epoch: 5 [52480/60000]\tLoss: 0.713453\n",
      "Train Epoch: 5 [53120/60000]\tLoss: 1.166769\n",
      "Train Epoch: 5 [53760/60000]\tLoss: 0.714596\n",
      "Train Epoch: 5 [54400/60000]\tLoss: 0.904114\n",
      "Train Epoch: 5 [55040/60000]\tLoss: 0.733027\n",
      "Train Epoch: 5 [55680/60000]\tLoss: 0.570681\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 0.953757\n",
      "Train Epoch: 5 [56960/60000]\tLoss: 0.745181\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.713725\n",
      "Train Epoch: 5 [58240/60000]\tLoss: 0.727243\n",
      "Train Epoch: 5 [58880/60000]\tLoss: 0.970400\n",
      "Train Epoch: 5 [59520/60000]\tLoss: 0.915461\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.803394\n",
      "Train Epoch: 6 [640/60000]\tLoss: 0.824417\n",
      "Train Epoch: 6 [1280/60000]\tLoss: 0.914035\n",
      "Train Epoch: 6 [1920/60000]\tLoss: 0.995008\n",
      "Train Epoch: 6 [2560/60000]\tLoss: 0.728512\n",
      "Train Epoch: 6 [3200/60000]\tLoss: 0.816865\n",
      "Train Epoch: 6 [3840/60000]\tLoss: 0.580882\n",
      "Train Epoch: 6 [4480/60000]\tLoss: 0.912396\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 0.860623\n",
      "Train Epoch: 6 [5760/60000]\tLoss: 0.573338\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 0.695090\n",
      "Train Epoch: 6 [7040/60000]\tLoss: 0.881253\n",
      "Train Epoch: 6 [7680/60000]\tLoss: 0.711403\n",
      "Train Epoch: 6 [8320/60000]\tLoss: 1.004207\n",
      "Train Epoch: 6 [8960/60000]\tLoss: 0.672046\n",
      "Train Epoch: 6 [9600/60000]\tLoss: 0.997360\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 0.784745\n",
      "Train Epoch: 6 [10880/60000]\tLoss: 0.878443\n",
      "Train Epoch: 6 [11520/60000]\tLoss: 0.727091\n",
      "Train Epoch: 6 [12160/60000]\tLoss: 0.671174\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.850624\n",
      "Train Epoch: 6 [13440/60000]\tLoss: 0.663482\n",
      "Train Epoch: 6 [14080/60000]\tLoss: 0.829768\n",
      "Train Epoch: 6 [14720/60000]\tLoss: 0.652787\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.685345\n",
      "Train Epoch: 6 [16000/60000]\tLoss: 1.125278\n",
      "Train Epoch: 6 [16640/60000]\tLoss: 0.895626\n",
      "Train Epoch: 6 [17280/60000]\tLoss: 0.639520\n",
      "Train Epoch: 6 [17920/60000]\tLoss: 0.896437\n",
      "Train Epoch: 6 [18560/60000]\tLoss: 0.834636\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 0.562383\n",
      "Train Epoch: 6 [19840/60000]\tLoss: 0.858085\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 0.679488\n",
      "Train Epoch: 6 [21120/60000]\tLoss: 0.856514\n",
      "Train Epoch: 6 [21760/60000]\tLoss: 0.748102\n",
      "Train Epoch: 6 [22400/60000]\tLoss: 0.843789\n",
      "Train Epoch: 6 [23040/60000]\tLoss: 0.797971\n",
      "Train Epoch: 6 [23680/60000]\tLoss: 0.754911\n",
      "Train Epoch: 6 [24320/60000]\tLoss: 0.950128\n",
      "Train Epoch: 6 [24960/60000]\tLoss: 0.692162\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.761115\n",
      "Train Epoch: 6 [26240/60000]\tLoss: 0.927863\n",
      "Train Epoch: 6 [26880/60000]\tLoss: 1.034544\n",
      "Train Epoch: 6 [27520/60000]\tLoss: 0.897440\n",
      "Train Epoch: 6 [28160/60000]\tLoss: 0.994624\n",
      "Train Epoch: 6 [28800/60000]\tLoss: 0.603286\n",
      "Train Epoch: 6 [29440/60000]\tLoss: 0.718191\n",
      "Train Epoch: 6 [30080/60000]\tLoss: 0.845208\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.823366\n",
      "Train Epoch: 6 [31360/60000]\tLoss: 0.781531\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.779491\n",
      "Train Epoch: 6 [32640/60000]\tLoss: 0.894203\n",
      "Train Epoch: 6 [33280/60000]\tLoss: 0.640708\n",
      "Train Epoch: 6 [33920/60000]\tLoss: 0.888385\n",
      "Train Epoch: 6 [34560/60000]\tLoss: 0.912878\n",
      "Train Epoch: 6 [35200/60000]\tLoss: 1.116519\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.720077\n",
      "Train Epoch: 6 [36480/60000]\tLoss: 0.680230\n",
      "Train Epoch: 6 [37120/60000]\tLoss: 0.732160\n",
      "Train Epoch: 6 [37760/60000]\tLoss: 0.471271\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 0.858098\n",
      "Train Epoch: 6 [39040/60000]\tLoss: 0.577456\n",
      "Train Epoch: 6 [39680/60000]\tLoss: 0.555569\n",
      "Train Epoch: 6 [40320/60000]\tLoss: 0.456881\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.836808\n",
      "Train Epoch: 6 [41600/60000]\tLoss: 1.062748\n",
      "Train Epoch: 6 [42240/60000]\tLoss: 0.701950\n",
      "Train Epoch: 6 [42880/60000]\tLoss: 0.585380\n",
      "Train Epoch: 6 [43520/60000]\tLoss: 0.750134\n",
      "Train Epoch: 6 [44160/60000]\tLoss: 0.826698\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.973527\n",
      "Train Epoch: 6 [45440/60000]\tLoss: 0.886531\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.646210\n",
      "Train Epoch: 6 [46720/60000]\tLoss: 0.798606\n",
      "Train Epoch: 6 [47360/60000]\tLoss: 0.776783\n",
      "Train Epoch: 6 [48000/60000]\tLoss: 0.790325\n",
      "Train Epoch: 6 [48640/60000]\tLoss: 0.924328\n",
      "Train Epoch: 6 [49280/60000]\tLoss: 0.615322\n",
      "Train Epoch: 6 [49920/60000]\tLoss: 0.996713\n",
      "Train Epoch: 6 [50560/60000]\tLoss: 0.812583\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.809490\n",
      "Train Epoch: 6 [51840/60000]\tLoss: 0.998300\n",
      "Train Epoch: 6 [52480/60000]\tLoss: 1.040159\n",
      "Train Epoch: 6 [53120/60000]\tLoss: 0.751676\n",
      "Train Epoch: 6 [53760/60000]\tLoss: 0.941348\n",
      "Train Epoch: 6 [54400/60000]\tLoss: 0.994022\n",
      "Train Epoch: 6 [55040/60000]\tLoss: 0.543511\n",
      "Train Epoch: 6 [55680/60000]\tLoss: 0.793728\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 0.731322\n",
      "Train Epoch: 6 [56960/60000]\tLoss: 0.731137\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.938119\n",
      "Train Epoch: 6 [58240/60000]\tLoss: 0.921768\n",
      "Train Epoch: 6 [58880/60000]\tLoss: 0.812571\n",
      "Train Epoch: 6 [59520/60000]\tLoss: 0.921780\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.595100\n",
      "Train Epoch: 7 [640/60000]\tLoss: 0.782557\n",
      "Train Epoch: 7 [1280/60000]\tLoss: 0.964882\n",
      "Train Epoch: 7 [1920/60000]\tLoss: 0.822591\n",
      "Train Epoch: 7 [2560/60000]\tLoss: 0.865622\n",
      "Train Epoch: 7 [3200/60000]\tLoss: 0.939265\n",
      "Train Epoch: 7 [3840/60000]\tLoss: 0.842051\n",
      "Train Epoch: 7 [4480/60000]\tLoss: 0.728402\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.646787\n",
      "Train Epoch: 7 [5760/60000]\tLoss: 0.751125\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 0.723275\n",
      "Train Epoch: 7 [7040/60000]\tLoss: 0.632687\n",
      "Train Epoch: 7 [7680/60000]\tLoss: 0.910345\n",
      "Train Epoch: 7 [8320/60000]\tLoss: 0.886302\n",
      "Train Epoch: 7 [8960/60000]\tLoss: 1.113505\n",
      "Train Epoch: 7 [9600/60000]\tLoss: 0.712189\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.896526\n",
      "Train Epoch: 7 [10880/60000]\tLoss: 0.945978\n",
      "Train Epoch: 7 [11520/60000]\tLoss: 0.762377\n",
      "Train Epoch: 7 [12160/60000]\tLoss: 0.623875\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 0.859154\n",
      "Train Epoch: 7 [13440/60000]\tLoss: 0.849149\n",
      "Train Epoch: 7 [14080/60000]\tLoss: 0.842307\n",
      "Train Epoch: 7 [14720/60000]\tLoss: 0.681506\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 0.769770\n",
      "Train Epoch: 7 [16000/60000]\tLoss: 0.661391\n",
      "Train Epoch: 7 [16640/60000]\tLoss: 0.828922\n",
      "Train Epoch: 7 [17280/60000]\tLoss: 0.619218\n",
      "Train Epoch: 7 [17920/60000]\tLoss: 0.895526\n",
      "Train Epoch: 7 [18560/60000]\tLoss: 1.112638\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.920289\n",
      "Train Epoch: 7 [19840/60000]\tLoss: 0.864109\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 0.926240\n",
      "Train Epoch: 7 [21120/60000]\tLoss: 0.615086\n",
      "Train Epoch: 7 [21760/60000]\tLoss: 0.940602\n",
      "Train Epoch: 7 [22400/60000]\tLoss: 0.839539\n",
      "Train Epoch: 7 [23040/60000]\tLoss: 0.899147\n",
      "Train Epoch: 7 [23680/60000]\tLoss: 1.029538\n",
      "Train Epoch: 7 [24320/60000]\tLoss: 0.770520\n",
      "Train Epoch: 7 [24960/60000]\tLoss: 0.921120\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.720156\n",
      "Train Epoch: 7 [26240/60000]\tLoss: 0.685757\n",
      "Train Epoch: 7 [26880/60000]\tLoss: 1.069207\n",
      "Train Epoch: 7 [27520/60000]\tLoss: 1.102118\n",
      "Train Epoch: 7 [28160/60000]\tLoss: 1.009853\n",
      "Train Epoch: 7 [28800/60000]\tLoss: 1.202120\n",
      "Train Epoch: 7 [29440/60000]\tLoss: 0.710499\n",
      "Train Epoch: 7 [30080/60000]\tLoss: 0.792600\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 1.261815\n",
      "Train Epoch: 7 [31360/60000]\tLoss: 0.995067\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.966444\n",
      "Train Epoch: 7 [32640/60000]\tLoss: 0.978788\n",
      "Train Epoch: 7 [33280/60000]\tLoss: 0.794442\n",
      "Train Epoch: 7 [33920/60000]\tLoss: 0.534234\n",
      "Train Epoch: 7 [34560/60000]\tLoss: 0.797438\n",
      "Train Epoch: 7 [35200/60000]\tLoss: 0.719501\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.759613\n",
      "Train Epoch: 7 [36480/60000]\tLoss: 0.817358\n",
      "Train Epoch: 7 [37120/60000]\tLoss: 0.783641\n",
      "Train Epoch: 7 [37760/60000]\tLoss: 0.849500\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 1.044199\n",
      "Train Epoch: 7 [39040/60000]\tLoss: 0.605357\n",
      "Train Epoch: 7 [39680/60000]\tLoss: 0.596729\n",
      "Train Epoch: 7 [40320/60000]\tLoss: 1.008776\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.768202\n",
      "Train Epoch: 7 [41600/60000]\tLoss: 1.023357\n",
      "Train Epoch: 7 [42240/60000]\tLoss: 0.821426\n",
      "Train Epoch: 7 [42880/60000]\tLoss: 1.237751\n",
      "Train Epoch: 7 [43520/60000]\tLoss: 0.776584\n",
      "Train Epoch: 7 [44160/60000]\tLoss: 0.884340\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.823904\n",
      "Train Epoch: 7 [45440/60000]\tLoss: 1.007609\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.930408\n",
      "Train Epoch: 7 [46720/60000]\tLoss: 0.640307\n",
      "Train Epoch: 7 [47360/60000]\tLoss: 0.882374\n",
      "Train Epoch: 7 [48000/60000]\tLoss: 0.761729\n",
      "Train Epoch: 7 [48640/60000]\tLoss: 0.904601\n",
      "Train Epoch: 7 [49280/60000]\tLoss: 0.625801\n",
      "Train Epoch: 7 [49920/60000]\tLoss: 1.133681\n",
      "Train Epoch: 7 [50560/60000]\tLoss: 1.054738\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.944030\n",
      "Train Epoch: 7 [51840/60000]\tLoss: 0.880551\n",
      "Train Epoch: 7 [52480/60000]\tLoss: 0.780369\n",
      "Train Epoch: 7 [53120/60000]\tLoss: 0.831211\n",
      "Train Epoch: 7 [53760/60000]\tLoss: 0.650445\n",
      "Train Epoch: 7 [54400/60000]\tLoss: 0.530632\n",
      "Train Epoch: 7 [55040/60000]\tLoss: 0.696739\n",
      "Train Epoch: 7 [55680/60000]\tLoss: 0.820072\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 0.881264\n",
      "Train Epoch: 7 [56960/60000]\tLoss: 0.771157\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 0.749260\n",
      "Train Epoch: 7 [58240/60000]\tLoss: 0.790033\n",
      "Train Epoch: 7 [58880/60000]\tLoss: 0.763682\n",
      "Train Epoch: 7 [59520/60000]\tLoss: 0.698851\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.667145\n",
      "Train Epoch: 8 [640/60000]\tLoss: 0.961574\n",
      "Train Epoch: 8 [1280/60000]\tLoss: 0.634593\n",
      "Train Epoch: 8 [1920/60000]\tLoss: 0.501189\n",
      "Train Epoch: 8 [2560/60000]\tLoss: 0.746046\n",
      "Train Epoch: 8 [3200/60000]\tLoss: 1.019400\n",
      "Train Epoch: 8 [3840/60000]\tLoss: 0.914307\n",
      "Train Epoch: 8 [4480/60000]\tLoss: 0.763398\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.692588\n",
      "Train Epoch: 8 [5760/60000]\tLoss: 0.782574\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.786107\n",
      "Train Epoch: 8 [7040/60000]\tLoss: 0.900342\n",
      "Train Epoch: 8 [7680/60000]\tLoss: 0.534073\n",
      "Train Epoch: 8 [8320/60000]\tLoss: 0.853699\n",
      "Train Epoch: 8 [8960/60000]\tLoss: 0.757193\n",
      "Train Epoch: 8 [9600/60000]\tLoss: 0.913396\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 0.609846\n",
      "Train Epoch: 8 [10880/60000]\tLoss: 1.080161\n",
      "Train Epoch: 8 [11520/60000]\tLoss: 1.076406\n",
      "Train Epoch: 8 [12160/60000]\tLoss: 0.694625\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.757827\n",
      "Train Epoch: 8 [13440/60000]\tLoss: 0.910650\n",
      "Train Epoch: 8 [14080/60000]\tLoss: 0.568636\n",
      "Train Epoch: 8 [14720/60000]\tLoss: 0.737734\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.794055\n",
      "Train Epoch: 8 [16000/60000]\tLoss: 0.635859\n",
      "Train Epoch: 8 [16640/60000]\tLoss: 0.530157\n",
      "Train Epoch: 8 [17280/60000]\tLoss: 0.929963\n",
      "Train Epoch: 8 [17920/60000]\tLoss: 0.826151\n",
      "Train Epoch: 8 [18560/60000]\tLoss: 0.761655\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 1.005900\n",
      "Train Epoch: 8 [19840/60000]\tLoss: 0.979865\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.798749\n",
      "Train Epoch: 8 [21120/60000]\tLoss: 0.771661\n",
      "Train Epoch: 8 [21760/60000]\tLoss: 0.704750\n",
      "Train Epoch: 8 [22400/60000]\tLoss: 0.716266\n",
      "Train Epoch: 8 [23040/60000]\tLoss: 0.902637\n",
      "Train Epoch: 8 [23680/60000]\tLoss: 0.862458\n",
      "Train Epoch: 8 [24320/60000]\tLoss: 0.669389\n",
      "Train Epoch: 8 [24960/60000]\tLoss: 0.801291\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.923070\n",
      "Train Epoch: 8 [26240/60000]\tLoss: 0.580074\n",
      "Train Epoch: 8 [26880/60000]\tLoss: 0.662107\n",
      "Train Epoch: 8 [27520/60000]\tLoss: 0.792884\n",
      "Train Epoch: 8 [28160/60000]\tLoss: 0.934467\n",
      "Train Epoch: 8 [28800/60000]\tLoss: 0.600212\n",
      "Train Epoch: 8 [29440/60000]\tLoss: 0.604733\n",
      "Train Epoch: 8 [30080/60000]\tLoss: 0.860338\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.687269\n",
      "Train Epoch: 8 [31360/60000]\tLoss: 0.942720\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 0.800704\n",
      "Train Epoch: 8 [32640/60000]\tLoss: 0.834670\n",
      "Train Epoch: 8 [33280/60000]\tLoss: 0.593472\n",
      "Train Epoch: 8 [33920/60000]\tLoss: 0.600215\n",
      "Train Epoch: 8 [34560/60000]\tLoss: 0.645295\n",
      "Train Epoch: 8 [35200/60000]\tLoss: 0.846446\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.920979\n",
      "Train Epoch: 8 [36480/60000]\tLoss: 1.013419\n",
      "Train Epoch: 8 [37120/60000]\tLoss: 1.038893\n",
      "Train Epoch: 8 [37760/60000]\tLoss: 1.410761\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 0.806578\n",
      "Train Epoch: 8 [39040/60000]\tLoss: 0.809119\n",
      "Train Epoch: 8 [39680/60000]\tLoss: 0.785600\n",
      "Train Epoch: 8 [40320/60000]\tLoss: 0.776922\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.997325\n",
      "Train Epoch: 8 [41600/60000]\tLoss: 0.738356\n",
      "Train Epoch: 8 [42240/60000]\tLoss: 0.760406\n",
      "Train Epoch: 8 [42880/60000]\tLoss: 0.983304\n",
      "Train Epoch: 8 [43520/60000]\tLoss: 1.009166\n",
      "Train Epoch: 8 [44160/60000]\tLoss: 0.804934\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 0.698312\n",
      "Train Epoch: 8 [45440/60000]\tLoss: 0.879728\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 1.014049\n",
      "Train Epoch: 8 [46720/60000]\tLoss: 1.020780\n",
      "Train Epoch: 8 [47360/60000]\tLoss: 0.903837\n",
      "Train Epoch: 8 [48000/60000]\tLoss: 0.699793\n",
      "Train Epoch: 8 [48640/60000]\tLoss: 1.062279\n",
      "Train Epoch: 8 [49280/60000]\tLoss: 0.741400\n",
      "Train Epoch: 8 [49920/60000]\tLoss: 0.636570\n",
      "Train Epoch: 8 [50560/60000]\tLoss: 0.883166\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.809387\n",
      "Train Epoch: 8 [51840/60000]\tLoss: 1.004868\n",
      "Train Epoch: 8 [52480/60000]\tLoss: 1.061141\n",
      "Train Epoch: 8 [53120/60000]\tLoss: 0.885145\n",
      "Train Epoch: 8 [53760/60000]\tLoss: 0.621717\n",
      "Train Epoch: 8 [54400/60000]\tLoss: 0.702523\n",
      "Train Epoch: 8 [55040/60000]\tLoss: 0.644311\n",
      "Train Epoch: 8 [55680/60000]\tLoss: 0.720135\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.795179\n",
      "Train Epoch: 8 [56960/60000]\tLoss: 0.772815\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.731303\n",
      "Train Epoch: 8 [58240/60000]\tLoss: 0.688062\n",
      "Train Epoch: 8 [58880/60000]\tLoss: 0.790744\n",
      "Train Epoch: 8 [59520/60000]\tLoss: 0.701500\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.628774\n",
      "Train Epoch: 9 [640/60000]\tLoss: 0.955536\n",
      "Train Epoch: 9 [1280/60000]\tLoss: 0.735629\n",
      "Train Epoch: 9 [1920/60000]\tLoss: 0.852861\n",
      "Train Epoch: 9 [2560/60000]\tLoss: 0.475238\n",
      "Train Epoch: 9 [3200/60000]\tLoss: 0.757089\n",
      "Train Epoch: 9 [3840/60000]\tLoss: 0.732337\n",
      "Train Epoch: 9 [4480/60000]\tLoss: 0.992023\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 0.891667\n",
      "Train Epoch: 9 [5760/60000]\tLoss: 1.018170\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 1.088398\n",
      "Train Epoch: 9 [7040/60000]\tLoss: 0.914337\n",
      "Train Epoch: 9 [7680/60000]\tLoss: 0.673255\n",
      "Train Epoch: 9 [8320/60000]\tLoss: 0.928982\n",
      "Train Epoch: 9 [8960/60000]\tLoss: 0.997398\n",
      "Train Epoch: 9 [9600/60000]\tLoss: 0.708013\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 0.857258\n",
      "Train Epoch: 9 [10880/60000]\tLoss: 0.875166\n",
      "Train Epoch: 9 [11520/60000]\tLoss: 0.882731\n",
      "Train Epoch: 9 [12160/60000]\tLoss: 0.792849\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.713751\n",
      "Train Epoch: 9 [13440/60000]\tLoss: 0.702947\n",
      "Train Epoch: 9 [14080/60000]\tLoss: 0.608043\n",
      "Train Epoch: 9 [14720/60000]\tLoss: 0.880747\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 1.100812\n",
      "Train Epoch: 9 [16000/60000]\tLoss: 0.803867\n",
      "Train Epoch: 9 [16640/60000]\tLoss: 0.947977\n",
      "Train Epoch: 9 [17280/60000]\tLoss: 0.707471\n",
      "Train Epoch: 9 [17920/60000]\tLoss: 0.769663\n",
      "Train Epoch: 9 [18560/60000]\tLoss: 0.793804\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.829603\n",
      "Train Epoch: 9 [19840/60000]\tLoss: 1.036047\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.747320\n",
      "Train Epoch: 9 [21120/60000]\tLoss: 0.865154\n",
      "Train Epoch: 9 [21760/60000]\tLoss: 0.768757\n",
      "Train Epoch: 9 [22400/60000]\tLoss: 0.601921\n",
      "Train Epoch: 9 [23040/60000]\tLoss: 0.864330\n",
      "Train Epoch: 9 [23680/60000]\tLoss: 0.589978\n",
      "Train Epoch: 9 [24320/60000]\tLoss: 0.488384\n",
      "Train Epoch: 9 [24960/60000]\tLoss: 0.934425\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.528151\n",
      "Train Epoch: 9 [26240/60000]\tLoss: 0.746418\n",
      "Train Epoch: 9 [26880/60000]\tLoss: 0.893409\n",
      "Train Epoch: 9 [27520/60000]\tLoss: 0.651731\n",
      "Train Epoch: 9 [28160/60000]\tLoss: 0.652353\n",
      "Train Epoch: 9 [28800/60000]\tLoss: 0.716731\n",
      "Train Epoch: 9 [29440/60000]\tLoss: 0.659021\n",
      "Train Epoch: 9 [30080/60000]\tLoss: 0.849141\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.872147\n",
      "Train Epoch: 9 [31360/60000]\tLoss: 0.794281\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.655238\n",
      "Train Epoch: 9 [32640/60000]\tLoss: 0.924190\n",
      "Train Epoch: 9 [33280/60000]\tLoss: 0.886353\n",
      "Train Epoch: 9 [33920/60000]\tLoss: 0.901214\n",
      "Train Epoch: 9 [34560/60000]\tLoss: 0.509727\n",
      "Train Epoch: 9 [35200/60000]\tLoss: 0.883233\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.588264\n",
      "Train Epoch: 9 [36480/60000]\tLoss: 1.014537\n",
      "Train Epoch: 9 [37120/60000]\tLoss: 0.724690\n",
      "Train Epoch: 9 [37760/60000]\tLoss: 0.613745\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.847571\n",
      "Train Epoch: 9 [39040/60000]\tLoss: 0.665710\n",
      "Train Epoch: 9 [39680/60000]\tLoss: 0.894444\n",
      "Train Epoch: 9 [40320/60000]\tLoss: 0.848451\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 0.789309\n",
      "Train Epoch: 9 [41600/60000]\tLoss: 0.795238\n",
      "Train Epoch: 9 [42240/60000]\tLoss: 0.894948\n",
      "Train Epoch: 9 [42880/60000]\tLoss: 0.876939\n",
      "Train Epoch: 9 [43520/60000]\tLoss: 0.845957\n",
      "Train Epoch: 9 [44160/60000]\tLoss: 0.852945\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.609431\n",
      "Train Epoch: 9 [45440/60000]\tLoss: 0.547857\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 0.819902\n",
      "Train Epoch: 9 [46720/60000]\tLoss: 0.781898\n",
      "Train Epoch: 9 [47360/60000]\tLoss: 0.926898\n",
      "Train Epoch: 9 [48000/60000]\tLoss: 0.842277\n",
      "Train Epoch: 9 [48640/60000]\tLoss: 0.771840\n",
      "Train Epoch: 9 [49280/60000]\tLoss: 0.779974\n",
      "Train Epoch: 9 [49920/60000]\tLoss: 0.638939\n",
      "Train Epoch: 9 [50560/60000]\tLoss: 0.817883\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.859237\n",
      "Train Epoch: 9 [51840/60000]\tLoss: 0.737784\n",
      "Train Epoch: 9 [52480/60000]\tLoss: 1.234496\n",
      "Train Epoch: 9 [53120/60000]\tLoss: 0.685343\n",
      "Train Epoch: 9 [53760/60000]\tLoss: 0.689970\n",
      "Train Epoch: 9 [54400/60000]\tLoss: 0.731414\n",
      "Train Epoch: 9 [55040/60000]\tLoss: 1.028691\n",
      "Train Epoch: 9 [55680/60000]\tLoss: 0.816615\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.965849\n",
      "Train Epoch: 9 [56960/60000]\tLoss: 0.595346\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.755098\n",
      "Train Epoch: 9 [58240/60000]\tLoss: 0.733594\n",
      "Train Epoch: 9 [58880/60000]\tLoss: 0.883621\n",
      "Train Epoch: 9 [59520/60000]\tLoss: 0.690068\n",
      "Train Epoch: 10 [0/60000]\tLoss: 0.650867\n",
      "Train Epoch: 10 [640/60000]\tLoss: 0.876458\n",
      "Train Epoch: 10 [1280/60000]\tLoss: 0.910002\n",
      "Train Epoch: 10 [1920/60000]\tLoss: 0.578135\n",
      "Train Epoch: 10 [2560/60000]\tLoss: 0.667397\n",
      "Train Epoch: 10 [3200/60000]\tLoss: 0.757133\n",
      "Train Epoch: 10 [3840/60000]\tLoss: 0.650972\n",
      "Train Epoch: 10 [4480/60000]\tLoss: 0.583358\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.941829\n",
      "Train Epoch: 10 [5760/60000]\tLoss: 0.629424\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.605655\n",
      "Train Epoch: 10 [7040/60000]\tLoss: 0.885185\n",
      "Train Epoch: 10 [7680/60000]\tLoss: 0.852252\n",
      "Train Epoch: 10 [8320/60000]\tLoss: 0.851458\n",
      "Train Epoch: 10 [8960/60000]\tLoss: 0.883325\n",
      "Train Epoch: 10 [9600/60000]\tLoss: 0.549668\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 0.499227\n",
      "Train Epoch: 10 [10880/60000]\tLoss: 0.696551\n",
      "Train Epoch: 10 [11520/60000]\tLoss: 0.836284\n",
      "Train Epoch: 10 [12160/60000]\tLoss: 0.859855\n",
      "Train Epoch: 10 [12800/60000]\tLoss: 0.690009\n",
      "Train Epoch: 10 [13440/60000]\tLoss: 0.667452\n",
      "Train Epoch: 10 [14080/60000]\tLoss: 0.788469\n",
      "Train Epoch: 10 [14720/60000]\tLoss: 0.812586\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 0.729557\n",
      "Train Epoch: 10 [16000/60000]\tLoss: 1.043724\n",
      "Train Epoch: 10 [16640/60000]\tLoss: 1.170611\n",
      "Train Epoch: 10 [17280/60000]\tLoss: 0.821222\n",
      "Train Epoch: 10 [17920/60000]\tLoss: 0.674216\n",
      "Train Epoch: 10 [18560/60000]\tLoss: 0.524122\n",
      "Train Epoch: 10 [19200/60000]\tLoss: 0.746044\n",
      "Train Epoch: 10 [19840/60000]\tLoss: 0.653752\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 0.877155\n",
      "Train Epoch: 10 [21120/60000]\tLoss: 0.621516\n",
      "Train Epoch: 10 [21760/60000]\tLoss: 0.965499\n",
      "Train Epoch: 10 [22400/60000]\tLoss: 0.826016\n",
      "Train Epoch: 10 [23040/60000]\tLoss: 1.005722\n",
      "Train Epoch: 10 [23680/60000]\tLoss: 0.666442\n",
      "Train Epoch: 10 [24320/60000]\tLoss: 0.478607\n",
      "Train Epoch: 10 [24960/60000]\tLoss: 0.551869\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 1.010368\n",
      "Train Epoch: 10 [26240/60000]\tLoss: 0.649702\n",
      "Train Epoch: 10 [26880/60000]\tLoss: 0.730948\n",
      "Train Epoch: 10 [27520/60000]\tLoss: 1.009941\n",
      "Train Epoch: 10 [28160/60000]\tLoss: 0.565190\n",
      "Train Epoch: 10 [28800/60000]\tLoss: 0.877036\n",
      "Train Epoch: 10 [29440/60000]\tLoss: 0.735811\n",
      "Train Epoch: 10 [30080/60000]\tLoss: 0.773057\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.850564\n",
      "Train Epoch: 10 [31360/60000]\tLoss: 0.924747\n",
      "Train Epoch: 10 [32000/60000]\tLoss: 0.743416\n",
      "Train Epoch: 10 [32640/60000]\tLoss: 0.710202\n",
      "Train Epoch: 10 [33280/60000]\tLoss: 0.700549\n",
      "Train Epoch: 10 [33920/60000]\tLoss: 0.736198\n",
      "Train Epoch: 10 [34560/60000]\tLoss: 0.800710\n",
      "Train Epoch: 10 [35200/60000]\tLoss: 0.666750\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.664130\n",
      "Train Epoch: 10 [36480/60000]\tLoss: 0.748910\n",
      "Train Epoch: 10 [37120/60000]\tLoss: 0.872032\n",
      "Train Epoch: 10 [37760/60000]\tLoss: 0.697460\n",
      "Train Epoch: 10 [38400/60000]\tLoss: 1.076820\n",
      "Train Epoch: 10 [39040/60000]\tLoss: 0.895393\n",
      "Train Epoch: 10 [39680/60000]\tLoss: 0.625589\n",
      "Train Epoch: 10 [40320/60000]\tLoss: 0.751741\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 0.717296\n",
      "Train Epoch: 10 [41600/60000]\tLoss: 0.605208\n",
      "Train Epoch: 10 [42240/60000]\tLoss: 0.746278\n",
      "Train Epoch: 10 [42880/60000]\tLoss: 0.653254\n",
      "Train Epoch: 10 [43520/60000]\tLoss: 1.016382\n",
      "Train Epoch: 10 [44160/60000]\tLoss: 0.798969\n",
      "Train Epoch: 10 [44800/60000]\tLoss: 0.903253\n",
      "Train Epoch: 10 [45440/60000]\tLoss: 1.001273\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.753341\n",
      "Train Epoch: 10 [46720/60000]\tLoss: 0.708210\n",
      "Train Epoch: 10 [47360/60000]\tLoss: 0.786401\n",
      "Train Epoch: 10 [48000/60000]\tLoss: 0.725389\n",
      "Train Epoch: 10 [48640/60000]\tLoss: 0.753193\n",
      "Train Epoch: 10 [49280/60000]\tLoss: 0.663349\n",
      "Train Epoch: 10 [49920/60000]\tLoss: 0.919374\n",
      "Train Epoch: 10 [50560/60000]\tLoss: 0.650722\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.842252\n",
      "Train Epoch: 10 [51840/60000]\tLoss: 0.880004\n",
      "Train Epoch: 10 [52480/60000]\tLoss: 0.968292\n",
      "Train Epoch: 10 [53120/60000]\tLoss: 0.836110\n",
      "Train Epoch: 10 [53760/60000]\tLoss: 0.954440\n",
      "Train Epoch: 10 [54400/60000]\tLoss: 0.877034\n",
      "Train Epoch: 10 [55040/60000]\tLoss: 0.510954\n",
      "Train Epoch: 10 [55680/60000]\tLoss: 0.788714\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.940081\n",
      "Train Epoch: 10 [56960/60000]\tLoss: 0.612584\n",
      "Train Epoch: 10 [57600/60000]\tLoss: 0.838343\n",
      "Train Epoch: 10 [58240/60000]\tLoss: 0.897018\n",
      "Train Epoch: 10 [58880/60000]\tLoss: 0.779160\n",
      "Train Epoch: 10 [59520/60000]\tLoss: 0.740950\n",
      "\n",
      "Test set: Avg. loss: 0.2129, Accuracy: 9413/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test Model 1\n",
    "\n",
    "# Create network\n",
    "model = Net()\n",
    "# Initialize model weights\n",
    "model.apply(weights_init)\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Get initial performance\n",
    "test(model)\n",
    "# Train for ten epochs\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, model)\n",
    "accuracy1 = test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2PVnghrmr0F",
    "outputId": "cde3cda1-7474-4e5b-c65e-2e5056b18853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3230, Accuracy: 1202/10000 (12%)\n",
      "\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.533287\n",
      "Train Epoch: 1 [640/60000]\tLoss: 2.177606\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 1.848610\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 1.902125\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 1.600636\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 1.428716\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 1.309780\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 1.263400\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 1.163735\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 0.908543\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 1.017123\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 0.864628\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 0.872780\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 0.762242\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 0.519485\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 0.758604\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 0.513923\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 0.454630\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 0.984696\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 0.589683\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 0.749695\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 0.560179\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 0.763014\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 0.290284\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 0.493582\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 0.421304\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 0.446246\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 0.340355\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 0.332484\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 0.465509\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 0.594540\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 0.683668\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 0.536636\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 0.611879\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 0.461231\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 0.535490\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 0.349777\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 0.632117\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 0.385918\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 0.662428\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 0.239749\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 0.457794\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 0.505698\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 0.283922\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 0.480399\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 0.376684\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 0.372108\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 0.264230\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 0.406108\n"
     ]
    }
   ],
   "source": [
    "# Train and test Model 2\n",
    "\n",
    "# Create network\n",
    "model2 = Net2()\n",
    "# Initialize model weights\n",
    "model2.apply(weights_init)\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Get initial performance\n",
    "test(model2)\n",
    "# Train for ten epochs\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, model2)\n",
    "accuracy2 = test(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFbCnAUmTwyx"
   },
   "source": [
    "## III. Results\n",
    "\n",
    "Here we train the CNN model and apply it to the test set. There are 10 epochs in training. There is no validation set here, we simply take the model at the end of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgAKHjLbqm3S",
    "outputId": "64ac15dc-4238-4c43-ec9a-e3edcaba71d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Accuracy: 94.05%\n",
      "Model 2 Accuracy: 98.79%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 1 Accuracy: {round(float(accuracy1.numpy()),2)}%\")\n",
    "print(f\"Model 2 Accuracy: {round(float(accuracy2.numpy()),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "8hG1l1rSulbg",
    "outputId": "f9cf5968-89bf-4372-b394-693b81f494fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/hqqt4z8949bf6xhx7jfq4f_40000gn/T/ipykernel_6233/2053953840.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run network on data we got before and show predictions\n",
    "output = model(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "zUHLA7qru5cQ",
    "outputId": "9037ddf9-fb9a-4c4f-c331-7329abc8a519"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAC+CAYAAABwHKjfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1GElEQVR4nO3deZxO9f//8cdgMJt9aERIqEiitFmzNpakklZUnyS74qtVRajUhybJEloUSfJJ02KKilbRQpQ0iiZbMUYMZub8/pjf+zqu2a9rruW4ruf9dnOb25zrLO9zXnMu7/M67yXCsiwLEREREXGEMsEugIiIiIjYVDkTERERcRBVzkREREQcRJUzEREREQdR5UxERETEQVQ5ExEREXEQVc5EREREHESVMxEREREHUeVMRERExEFOucpZ/fr1GThwoOv3NWvWEBERwZo1a3x2jIiICB555BGf7U+8o1iHD8U6PCjO4UOxLh2PKmcLFy4kIiLC9a9ixYo0btyYYcOGsWfPHn+V0S+Sk5NPmaCefM3z/uvSpYtfjqlYB88bb7zBJZdcQpUqVahevTrt27fn3Xff9dvxFOvAy8nJYeHChfTu3Zu6desSExNDs2bNmDRpEpmZmX45puIcHHPnzqV9+/bUqlWLChUq0KBBAwYNGsSOHTv8dkzFOji+/vpr7r77blq1akVkZCQRERFe76ucNxs99thjNGjQgMzMTNauXcusWbNITk5m06ZNREdHe10Yb7Rr146jR49Svnx5j7ZLTk5m5syZBQb96NGjlCvn1aXxi1deeSXfsvXr1zNjxgy6du3q12Mr1oGVlJTEiBEj6NGjB1OnTiUzM5OFCxfSs2dPli1bRt++ff12bMU6cI4cOcKgQYO45JJLuOuuu6hZsyZffPEFEyZM4KOPPuLjjz8u1Rd7URTnwNq4cSMNGjSgd+/eVK1aldTUVObOncvKlSv5/vvvqV27tt+OrVgHVnJyMvPmzaN58+aceeaZ/PLLL97vzPLAggULLMD65ptv3JaPGTPGAqzXXnut0G0PHz7syaEKVa9ePWvAgAGl3s/QoUMtD0/fUW6//XYrIiLC2rlzp1/2r1gHR6NGjayLLrrIysnJcS1LT0+3YmNjrd69e/vlmIp14B07dsxat25dvuWPPvqoBVirVq3y+TEVZ+dYv369BVhTpkzxy/4V6+DYvXu3deTIEcuySl9un7Q5u+KKKwBITU0FYODAgcTGxrJ9+3YSExOJi4vjpptuAnLT+dOnT6dp06ZUrFiRWrVqMXjwYA4cOJC30sikSZOoU6cO0dHRdOzYkc2bN+c7dmHvsb/66isSExOpWrUqMTExNG/enBkzZrjKN3PmTMD9laFR0HvsjRs3cuWVV1KpUiViY2Pp1KkTX375pds6JpW8bt06xowZQ3x8PDExMVx99dXs27fPbd309HS2bt1Kenp6SS6xm2PHjrFs2TLat29PnTp1PN6+NBTrXP6K9aFDh6hZs6ZbGU05oqKiit3elxTrXP6Idfny5bnsssvyLb/66qsB2LJlS5Hb+5LinCtQ39+Q2x4L4ODBg15t7y3FOpe/Yl2rVi2ffU/7JB+4fft2AKpXr+5alpWVRbdu3WjTpg3Tpk1zpVAHDx7MwoULGTRoECNGjCA1NZXnnnuOjRs3sm7dOiIjIwF4+OGHmTRpEomJiSQmJrJhwwa6du3K8ePHiy3PqlWr6NmzJwkJCYwcOZLTTjuNLVu2sHLlSkaOHMngwYNJS0tj1apVBb4yzGvz5s20bduWSpUqMW7cOCIjI5k9ezYdOnTgk08+4eKLL3Zbf/jw4VStWpUJEyawY8cOpk+fzrBhw1iyZIlrneXLlzNo0CAWLFjg1miyJJKTkzl48KDrJgokxdq/se7QoQNvvvkmSUlJ9OrVi8zMTJKSkkhPT2fkyJHFlt+XFOvA3tcAu3fvBqBGjRoeb+stxTkwcf7777/Jzs7mjz/+4LHHHgOgU6dOJdrWVxTrwN/TXvMkzWZSpSkpKda+ffusnTt3WosXL7aqV69uRUVFWbt27bIsy7IGDBhgAdb48ePdtv/ss88swFq0aJHb8vfff99t+d69e63y5ctbPXr0cHu9c//991uAW6p09erVFmCtXr3asizLysrKsho0aGDVq1fPOnDggNtxTt5XUSlHwJowYYLr9z59+ljly5e3tm/f7lqWlpZmxcXFWe3atct3fTp37ux2rNGjR1tly5a1Dh48mG/dBQsWFFiGolxzzTVWhQoV8p2fLynWwYn1nj17rE6dOlmA61+NGjWszz//vNhtvaVYO+O+tizL6ty5s1WpUiW/3NuKc3DjXKFCBdc9Xb16devZZ58t8baeUqyDf08H5bVm586diY+Pp27duvTv35/Y2FiWL1/O6aef7rbekCFD3H5funQplStXpkuXLuzfv9/1r1WrVsTGxrJ69WoAUlJSOH78OMOHD3dLYY4aNarYsm3cuJHU1FRGjRpFlSpV3D7zpoFtdnY2H374IX369OHMM890LU9ISODGG29k7dq1HDp0yG2bO++80+1Ybdu2JTs7m99//921bODAgViW5XFN/NChQ7z77rskJibmOz9/UKwDG+vo6GiaNGnCgAEDWLp0KfPnzychIYG+ffvy66+/enxOnlCsg3dfA0yePJmUlBSmTp3q13tbcQ5OnN977z2Sk5N5+umnOeOMM/j33389Ph9PKdbBvadLw6vXmjNnzqRx48aUK1eOWrVq0aRJE8qUca/nlStXLl97qG3btpGenk7NmjUL3O/evXsBXBemUaNGbp/Hx8dTtWrVIstm0rbNmjUr+QkVYd++fRw5coQmTZrk++ycc84hJyeHnTt30rRpU9fyM844w209U+a87+q9sWzZMjIzMwP2SlOxzhWoWF933XWUK1eOd955x7XsqquuolGjRjzwwANu6XZfU6xzBeO+XrJkCQ8++CC33357vv8ofU1xzhXoOHfs2BGAK6+8kquuuopmzZoRGxvLsGHDSrXfoijWuYJxT5eWV5Wz1q1bc+GFFxa5ToUKFfL9EeTk5FCzZk0WLVpU4Dbx8fHeFMdxypYtW+Byy7JKve9FixZRuXJlevbsWep9lYRiXTRfxvq3337j/fffZ86cOW7Lq1WrRps2bVi3bp1XZSwpxbpo/rqvV61axa233kqPHj144YUXSrWvklCci+bP72+jYcOGXHDBBSxatMivlTPFumiBiLW3AjpASMOGDUlJSeHyyy8vskdDvXr1gNza+8npyX379hVbo23YsCEAmzZtonPnzoWuV9K0aXx8PNHR0fz888/5Ptu6dStlypShbt26JdpXaf3111+sXr2agQMHUqFChYAc01uKtefM4JDZ2dn5Pjtx4gRZWVl+O3ZpKNbe++qrr7j66qu58MILeeONNxw1ZlNeirNvHT16lGPHjgXl2MVRrIMvoNM39evXj+zsbCZOnJjvs6ysLFe34s6dOxMZGUlSUpJbDXb69OnFHqNly5Y0aNCA6dOn5+umfPK+YmJigOK7MpctW5auXbuyYsUKtxGd9+zZw2uvvUabNm2oVKlSseXKy5uu2IsXLyYnJycovTQ9pVjbShrrs846izJlyrBkyRK38u/atYvPPvuMCy64wONjB4JibfPkvt6yZQs9evSgfv36rFy5MuBDpXhKcbaVNM5ZWVkFVlK+/vprfvzxx2KzWsGiWNtKO2yKtwL6mNa+fXsGDx7MlClT+O677+jatSuRkZFs27aNpUuXMmPGDK699lri4+O59957mTJlCj179iQxMZGNGzfy3nvvFdvFvEyZMsyaNYtevXrRokULBg0aREJCAlu3bmXz5s188MEHALRq1QqAESNG0K1bN8qWLUv//v0L3OekSZNYtWoVbdq04e6776ZcuXLMnj2bY8eO8eSTT3p1Lbzpnrto0SJq165Nhw4dvDpmICnWtpLGOj4+nttuu4158+bRqVMn+vbtS0ZGBs8//zxHjx7lvvvu8+r4/qZY20oa64yMDLp168aBAwcYO3Zsvum5GjZsyKWXXupVGfxFcbaVNM6HDx+mbt26XH/99TRt2pSYmBh+/PFHFixYQOXKlXnooYe8Or6/KdY2T/6v/v33311Dfqxfv95VJsjNMt5yyy0lP7AnXTsLG3U4rwEDBlgxMTGFfj5nzhyrVatWVlRUlBUXF2edd9551rhx46y0tDTXOtnZ2dajjz5qJSQkWFFRUVaHDh2sTZs25Rt1OG/3XGPt2rVWly5drLi4OCsmJsZq3ry5lZSU5Po8KyvLGj58uBUfH29FRES4dXklT/dcy7KsDRs2WN26dbNiY2Ot6Ohoq2PHjvmGNyjs+hRURk+7527dutUCrDFjxpRo/dJSrIMT6xMnTlhJSUlWixYtrNjYWCs2Ntbq2LGj9fHHHxe7rbcU68DHOjU11W24lLz/fDGyel6Kc+DjfOzYMWvkyJFW8+bNrUqVKlmRkZFWvXr1rNtvv91KTU0tctvSUKyD8/1tti/oX/v27Yvd/mQR//8ERURERMQBAtrmTERERESKpsqZiIiIiIOociYiIiLiIKqciYiIiDiIKmciIiIiDqLKmYiIiIiDOGaukJycHNLS0oiLi/NqRnqnsiyLjIwMateunW/+snClWIeHUI0zKNZ5KdbhQ7EODMdUztLS0k6pea88tXPnTurUqRPsYjiCYh0eQj3OoFgbinX4UKwDwzGPAXFxccEugl+F+vl5ItSvRaifX0mFw3UIh3MsiXC4DuFwjiURDtfBCefomMpZqKVH8wr18/NEqF+LUD+/kgqH6xAO51gS4XAdwuEcSyIcroMTztExlTMRERERUeVMRERExFFUORMRERFxEMf01hQRESlM7969AVixYgW7d+8GYOnSpQA8//zzAGzdujU4hRPxMWXORERERBxEmTMJCU2aNAHgnHPOcVtevnx5lixZAuQOMFiQvn37AvD222/7r4Ai4pXRo0cD8MQTTwC593GtWrUAGDZsGAC33HILAG3btgVg06ZNgS6mBEjLli0BSElJAXANFlulSpVgFckvlDkTERERcRBlzuSUZp6mO3XqBMAFF1yQb52cnJwi9zF+/HhAmTMRJ2nYsCEAU6dOBeCzzz4D4IEHHnCtY+7ZmjVrAnbmXJmz0HP++ecDuW0Owc6UHT58GIDatWsDuTMYhAJlzkREREQcJKQzZ4888ghgZ1XeffddADZs2OBa55NPPgHg2LFjgS2clMrEiRMBGDNmDIBHk9QeOHAAgB9//BGAAQMG+Lh0Eihly5YFICoqyvUELaHh+PHjALz00ksA3HPPPQBkZGTkW0dCn/nOP/30092Wx8bGAjBq1CgAxo0bF9By+YsyZyIiIiIOEpKZs8mTJwMwdOhQALZv3w7YbRWio6Ndc2ft3bsXsNsoLFu2DIDFixcDdpZFnMG0Q+nevTvgWcbM+PrrrwFITEwE4IwzzgBg2rRpAPz9998ATJkypXSFFZ+75JJLALsdUoMGDQCIjIxkz549gH0Pm3WysrICXUzxgZ07dwJw55135vvM3P+VK1cGcMX++++/D1DpxCnS09MBuP/++4NcEt9S5kxERETEQUIyc2bGQXnwwQcBSEpKAqBx48YAVKxYkYSEBACuueYawB59umPHjgDceuutAHTu3BmAf//9NxBFl0KYXpk33ngjYPfMyctkSd966y0A5syZk2+do0ePuu3jnXfeAaBZs2YAnDhxwm19ZdCCp0aNGgD873//A6BFixYAVKhQAcCVAbcsi+zsbAAeffRRwP5beP311wNWXvG/li1bkpyc7Lasf//+APzyyy/BKJL4kWlTZn4WJtQy5MqciYiIiDhISGbOjGrVqrn9fvJT1Q8//ADABx98AMDw4cMBWLt2LQB16tQBoFy5kL5Ep4z4+Hig8IyZYTJr69evL3Qd027t1VdfBeyMmTF//nwANm7c6F1hpdS6desGwHvvvVfg5yaT/cwzzwC5cyyeeeaZgD321XXXXQcocxYqLrzwQgC3rNmff/4JwJdffhmUMon/XXHFFQB06NChwM/N/KqhRpkzEREREQcJybTQokWLALj55ptLvI0Z5+yff/5x+2l6gsipwWTYzOjRBw8edGVhzDg5lSpVAqBRo0YA/PTTTwAMGjQIgM2bNwN22zQJPDNmWWpqKmDfh6Yn5osvvgjA7t27Xdtce+21gD2HamFzqcqpxdy3ZvyqyMhI12em7fBHH30EwJNPPgnYfyd524/KqaVZs2bMnj27wM9MD13zpiPUKHMmIiIi4iAhmTkz8yvOnTu3xNuYnpzt2rUD4JZbbvF9wcTvVq5cCcCnn34KwBtvvEGrVq0AXD/zMk/ZRbVTk8Bat24dYLcPLAmTETUOHTrk0zJJYJk2RmbuW5MJXbNmjWvss9tvvx2wR4d/7bXXAJg1axZgj2158ODBQBRZfOz666+nVq1aBX5m3nSEantDZc5EREREHCTCckjDjEOHDrlGew4k035h3rx5gN1OzczZ5yvp6en5nuzDlTexbtq0KQAff/wxYLcp86Y3rRkP69lnnwXsp2tfza+qWOcK5D1do0YNtmzZAti9tK+88koAPvzwQ78dV7HO5c9Yt2/fHrDbj33++ef51rn44osBu2du/fr1Adi2bRtgz6+8a9cur8uhWOcK5H09ceJE1/dzXibGf/zxh8+P64RYK3MmIiIi4iAh2ebME+eeey5gZ8xSUlKCWRwphOlBadofmHkwR48e7fG+TIzvvfdeH5VOgq1Nmzb5nuY3bNgQpNKIL33yySfFrvPVV18BuMa6W7FiBQC9evUC4OmnnwZy2zCJ80VHRwN2W/BwFNKVMzO46Pnnnw/YQ2wAdO3aFbAHuDPTwHTp0gXI3x3fNDQ3jcdXrVoFQFpamv9OQAplBpA1QymY6Z1Kwvw9mFcgv/76KwAPPfSQL4soAVC+fHkgd9q1vK+4zcDSZjgOE2czOXZGRkagiikBNnLkSMAeuDYxMRGASy65BAjdRuSh4qyzzgLsgaRPZgagNs1TQpVea4qIiIg4SEhnzsaOHQvYk+I+9dRTrs9OO+00wM6MHT9+HIB3330XsBudmobnhpmI2SxX5iw4vvvuOwB+++03AJo3bw7ATTfdVOy2Jvb9+vUDICcnB4B77rkHsLvna9of56lYsSJgN/YfNmwYkDvsQt6+TQ8++CDgPjk62APcLly4ELAza2+99RbgnwbGElg7duwA4KWXXgLs4ThM1lyZM2cyzYzMG6qC/Pe//wXsqbtClTJnIiIiIg4S0pkzM2myaZdkGouCnS0xU76YKSJMWwU5NZxxxhlA4RmzpKQktm7d6rZsxowZgD0MR5kyuc8oFSpUAGDmzJmAnU0t6ilOAqN169YAPProo4DdZvTkrNjPP/8MwM6dOwE7+222NRlT89Nk3QyTWTdZFZN5++KLLwD770FOHfv27Qt2EcQDpp14QYNP79+/H4BnnnkmoGUKFmXORERERBwkpDNnixcvdvt5sqlTpwJw4MABwB6aQU4NJmN2cg/ck+3duxfIbTdmutkbv//+O2BPmFuzZk23z82QDAsWLADsdk6FHUt8z2Q158yZA8ANN9wA2L0z85o/fz5Dhw4Fis9wmfai5unc9Oo0GbbLLrsMsAc8Nm1bJkyYANh/F+JcJgOTd6gd830vp57nn38eCJ+puJQ5ExEREXGQkM6cFcVMgm3ap5ifcmoYMWIEYD8h5/Xmm28C5Muagd0W0fTOu+uuuwrcR0xMDADTp08HlDkLJDNY6IABAwr83PSSNlnxhx9+uMRtwsyT97fffgvAwIEDATveph2qyaSZv4/HHnsMUObMVx555BHA/i42vaePHj3q9T7PPvtsAJKTkwGoU6eO2+fvvPOO1/uW4DL3a7hQ5kxERETEQcIyc9aqVSvXRLhr1qwJbmHEr8qUKePWS/dkwZ7YVgpnZoDIO3ZZVlYWYM/kkbcnbmmYkcfN2Fjmp2nLJr5lpubp0aMHAFFRUYBnmTOTOe/Tpw8AQ4YMASAhIQGAY8eOATBq1ChAPW6dxkzTZGZ4ueaaa4JZHEdR5kxERETEQcIyc3bNNde4nqjuvPPOIJdG/KFFixYAjBs3jscffzy4hRGPmVH7TTZr3bp1ALzwwguAbzNmEhyffvopYM/gsnz5cgB++uknwO4ta76rzawQDRs2pE2bNgCULVsWgMjISLd9f/DBBwCue3/t2rX+OQkplbp16wKFZ6cjIiLyZc/DhTJnIiIiIg4SVpkz0yvojjvu4JNPPgHg119/DWaRxE/MWFXmZ2mYuRglcAprJyih44cffgDs7Gjbtm3dfg4ePLjYfZw4cQKAjz76CLBn85g7dy4A2dnZPiyxBFq4Zs1AmTMRERERRwmrzNmNN94IQI0aNTQjgBRr06ZNgHoQifiDmfv00ksvBaBJkyYA9OzZE4D69esDcM455wD2+IQAL7/8MmD37DTzocqpJTMzE7Dnvzbz3p7MvOUyP8OFMmciIiIiDhIWmTMzfo4ZT2f9+vWkpKQEs0hSSmaU9h07dgAwY8YMr/e1bds2ILdn58m2b98OqF2iiD+ZDLX5adqNSegz8xyb2Vfuuecet8/ffvttVw/tQ4cOBbZwQabMmYiIiIiDhEXmbMyYMQA0atQIsHvyyKlr8+bNbj+fe+65YBZHRES8NHbsWLefosyZiIiIiKOERebsn3/+AeyRp5VlEREREadS5kxERETEQcIiczZr1iy3nyIiIiJOpcyZiIiIiIOociYiIiLiII6pnIX6BKehfn6eCPVrEernV1LhcB3C4RxLIhyuQzicY0mEw3Vwwjk6pnKWkZER7CL4VaifnydC/VqE+vmVVDhch3A4x5IIh+sQDudYEuFwHZxwjhGWE6qIQE5ODmlpacTFxRERERHs4viMZVlkZGRQu3ZtypRxTF04qBTr8BCqcQbFOi/FOnwo1oHhmMqZiIiIiDjotaaIiIiIqHImIiIi4iiqnImIiIg4iCpnIiIiIg6iypmIiIiIg6hyJiIiIuIgqpyJiIiIOIgqZyIiIiIOosqZiIiIiIOociYiIiLiIKqciYiIiDiIKmciIiIiDqLKmYiIiIiDnHKVs/r16zNw4EDX72vWrCEiIoI1a9b47BgRERE88sgjPtufeEexDh+KdXhQnMOHYl06HlXOFi5cSEREhOtfxYoVady4McOGDWPPnj3+KqNfJCcnn1JBfeONN7jkkkuoUqUK1atXp3379rz77rt+O55iHTxbtmyhe/fuxMbGUq1aNW655Rb27dvnt+Mp1sETyPtacQ6OgQMHul138+/ss8/22zEV6+A7ceIE5557LhEREUybNs3j7ct5c9DHHnuMBg0akJmZydq1a5k1axbJycls2rSJ6Ohob3bptXbt2nH06FHKly/v0XbJycnMnDmzwKAfPXqUcuW8ujR+kZSUxIgRI+jRowdTp04lMzOThQsX0rNnT5YtW0bfvn39dmzFOrB27dpFu3btqFy5MpMnT+bw4cNMmzaNH3/8ka+//trjc/eEYh1YwbqvFefAq1ChAvPmzXNbVrlyZb8fV7EOnqSkJP744w/vd2B5YMGCBRZgffPNN27Lx4wZYwHWa6+9Vui2hw8f9uRQhapXr541YMCAUu9n6NChloenHzSNGjWyLrroIisnJ8e1LD093YqNjbV69+7tl2Mq1sExZMgQKyoqyvr9999dy1atWmUB1uzZs/1yTMU6OAJ9XyvOwTFgwAArJiYmoMdUrINrz549VuXKla3HHnvMAqynnnrK4334pM3ZFVdcAUBqaiqQm8aNjY1l+/btJCYmEhcXx0033QRATk4O06dPp2nTplSsWJFatWoxePBgDhw4kLfSyKRJk6hTpw7R0dF07NiRzZs35zt2Ye+xv/rqKxITE6latSoxMTE0b96cGTNmuMo3c+ZMALfUr1HQe+yNGzdy5ZVXUqlSJWJjY+nUqRNffvml2zomlbxu3TrGjBlDfHw8MTExXH311fleS6Wnp7N161bS09OLvb6HDh2iZs2abmU05YiKiip2e19SrHP5K9bLli2jZ8+enHHGGa5lnTt3pnHjxrzxxhvFbu9LinWuUL+vFedc/oqzkZ2dzaFDh0q8vj8o1rn8Hevx48fTpEkTbr755hJvk5dP8oHbt28HoHr16q5lWVlZdOvWjTZt2jBt2jRXCnXw4MEsXLiQQYMGMWLECFJTU3nuuefYuHEj69atIzIyEoCHH36YSZMmkZiYSGJiIhs2bKBr164cP3682PKsWrWKnj17kpCQwMiRIznttNPYsmULK1euZOTIkQwePJi0tDRWrVrFK6+8Uuz+Nm/eTNu2balUqRLjxo0jMjKS2bNn06FDBz755BMuvvhit/WHDx9O1apVmTBhAjt27GD69OkMGzaMJUuWuNZZvnw5gwYNYsGCBW6NJgvSoUMH3nzzTZKSkujVqxeZmZkkJSWRnp7OyJEjiy2/LynW/ov1n3/+yd69e7nwwgvzfda6dWuSk5OLLb8vKdbhcV8rzv6NM8CRI0eoVKkSR44coWrVqtxwww088cQTxMbGFrutLynW/o/1119/zUsvvcTatWvdKpIe8yTNZlKlKSkp1r59+6ydO3daixcvtqpXr25FRUVZu3btsiwrN40LWOPHj3fb/rPPPrMAa9GiRW7L33//fbfle/futcqXL2/16NHDLeV///33W4BbqnT16tUWYK1evdqyLMvKysqyGjRoYNWrV886cOCA23FO3ldRqVLAmjBhguv3Pn36WOXLl7e2b9/uWpaWlmbFxcVZ7dq1y3d9Onfu7Has0aNHW2XLlrUOHjyYb90FCxYUWIaT7dmzx+rUqZMFuP7VqFHD+vzzz4vd1luKdeBj/c0331iA9fLLL+f7bOzYsRZgZWZmFrkPbyjW4XFfK87BifP48eOt//u//7OWLFlivf76667re/nll1snTpwodntvKNbBiXVOTo7VunVr64YbbrAsy7JSU1MD+1qzc+fOxMfHU7duXfr3709sbCzLly/n9NNPd1tvyJAhbr8vXbqUypUr06VLF/bv3+/616pVK2JjY1m9ejUAKSkpHD9+nOHDh7vVPEeNGlVs2TZu3EhqaiqjRo2iSpUqbp95U4vNzs7mww8/pE+fPpx55pmu5QkJCdx4442sXbs2X6r6zjvvdDtW27Ztyc7O5vfff3ctGzhwIJZllagmHh0dTZMmTRgwYABLly5l/vz5JCQk0LdvX3799VePz8kTinXgYn306FEgt/FwXhUrVnRbxx8U6/C4rxXnwMZ5ypQpTJ06lX79+tG/f38WLlzI448/zrp163jzzTc9PidPKNaBjfXChQv58ccfeeKJJzwuf15evdacOXMmjRs3ply5ctSqVYsmTZpQpox7Pa9cuXLUqVPHbdm2bdtIT0+nZs2aBe537969AK4L06hRI7fP4+PjqVq1apFlM2nbZs2alfyEirBv3z6OHDlCkyZN8n12zjnnkJOTw86dO2natKlr+cnthQBXmfO+qy+p6667jnLlyvHOO++4ll111VU0atSIBx54wC0F62uKda5AxNq0Mzp27Fi+zzIzM93W8QfFOleo39eKc65Axbkgo0eP5qGHHiIlJYX+/fv7bL95Kda5AhHrQ4cOcd999zF27Fjq1q3r8fZ5eVU5a926dYHtYk5WoUKFfH8EOTk51KxZk0WLFhW4TXx8vDfFcZyyZcsWuDw3C+uZ3377jffff585c+a4La9WrRpt2rRh3bp1XpWxpBTrovky1gkJCQD89ddf+T7766+/qFatWoFZNV9RrIsWKve14lw0X8a5MFFRUVSvXp1//vnHZ/ssiGJdNF/Getq0aRw/fpzrr7+eHTt2ALlDI0FuZW/Hjh3Url27xEOJBHSAkIYNG5KSksLll19eZAagXr16QG7t/eT05L59+4qt0TZs2BCATZs20blz50LXK2naND4+nujoaH7++ed8n23dupUyZcr4pJZcGDNgYHZ2dr7PTpw4QVZWlt+OXRqKtedOP/104uPjWb9+fb7Pvv76a1q0aOG3Y5eGYu25U/G+Vpx9JyMjg/379zu2kqNYe+6PP/7gwIEDbpk5Y/LkyUyePJmNGzeW+Hs8oNM39evXj+zsbCZOnJjvs6ysLA4ePAjkviePjIwkKSnJrQY7ffr0Yo/RsmVLGjRowPTp0137M07eV0xMDEC+dfIqW7YsXbt2ZcWKFa7aMOR+ub722mu0adOGSpUqFVuuvEraPfess86iTJkyLFmyxK38u3bt4rPPPuOCCy7w+NiBoFjbPOmKfc0117By5Up27tzpWvbRRx/xyy+/cN1113l87EBQrG2hfF8rzraSxjkzM5OMjIx8yydOnIhlWXTv3t3jYweCYm0raaxHjBjB8uXL3f7Nnj0byG23tnz5cho0aFDi4wY0c9a+fXsGDx7MlClT+O677+jatSuRkZFs27aNpUuXMmPGDK699lri4+O59957mTJlCj179iQxMZGNGzfy3nvvUaNGjSKPUaZMGWbNmkWvXr1o0aIFgwYNIiEhga1bt7J582Y++OADAFq1agXkXtBu3bpRtmzZQt/9T5o0iVWrVtGmTRvuvvtuypUrx+zZszl27BhPPvmkV9eipN1z4+Pjue2225g3bx6dOnWib9++ZGRk8Pzzz3P06FHuu+8+r47vb4q1zZOu2Pfffz9Lly6lY8eOjBw5ksOHD/PUU09x3nnnMWjQIK+O72+KtS2U72vF2VbSOO/evZsLLriAG264wTVd0wcffEBycjLdu3fnqquu8ur4/qZY20oa65YtW9KyZUu3ZaaS2LRpU/r06ePZgT3p2lnYqMN5FTci8pw5c6xWrVpZUVFRVlxcnHXeeedZ48aNs9LS0lzrZGdnW48++qiVkJBgRUVFWR06dLA2bdqUb9ThvN1zjbVr11pdunSx4uLirJiYGKt58+ZWUlKS6/OsrCxr+PDhVnx8vBUREeHWVZc83XMty7I2bNhgdevWzYqNjbWio6Otjh075uvyXtj1KaiMnnTPPXHihJWUlGS1aNHCio2NtWJjY62OHTtaH3/8cbHbekuxDk6sLcuyNm3aZHXt2tWKjo62qlSpYt10003W7t27S7StNxTr8LivFefAx/nAgQPWzTffbJ111llWdHS0VaFCBatp06bW5MmTrePHjxe5bWko1sH7/j5ZaYbSiLAsH7ZyFBEREZFSCWibMxEREREpmipnIiIiIg6iypmIiIiIg6hyJiIiIuIgqpyJiIiIOIgqZyIiIiIOEtBBaIuSk5NDWloacXFxXs1I71SWZZGRkUHt2rXzzV8WrhTr8BCqcQbFOi/FOnwo1oHhmMpZWlpaUOY4C5SdO3dSp06dYBfDERTr8BDqcQbF2lCsw4diHRiOeQyIi4sLdhH8KtTPzxOhfi1C/fxKKhyuQzicY0mEw3UIh3MsiXC4Dk44R8dUzkItPZpXqJ+fJ0L9WoT6+ZVUOFyHcDjHkgiH6xAO51gS4XAdnHCOjqmciYiIiIgqZyIiIiKOosqZiIiIiIOociYiIiLiIKqciYiIiDiIY8Y5ExERKc4dd9zBZZddBsCgQYM82vbvv/8GoEaNGj4vlwTWzJkzARgyZAhA0AeN9bXQOhsRERGRU1zYZs7Kli0LwNNPPw3AbbfdBtiDz917770APPPMM0DutA4S2v73v/8B0L59e7fl5m9h7ty5AS+TFMxkTl588UUAzj777HzrfPzxxwA8++yzAKxYsSJApRNf+s9//gPAQw89BMDpp5/uGofK0+/lqKgoAC655BIAvvzyS18VUwIsNTUVCN3/m5U5ExEREXGQsMqcmXfSzZs3Z9asWQBcfPHFABw7dgyAI0eOADBlyhQANm3aBNhPWmb58ePHA1RqCRTzVB0bG+u2/LHHHgPgiy++AOy/CQm8Nm3aADB9+nQAKlWqBMCMGTNc6/Tr1w+Ajh07AnDixAkAvv/+ewB27NgRiKKKh5o3bw7Ymeo+ffoAEB0dDXjXpuiDDz4AYOvWrQAcPnwYUMbMKcx37tGjR4NcEudR5kxERETEQUI6c3b11VcD0K1bNwAaNWoEQOvWrV1PVDfeeCMA77zzDgD//vsvAOeeey4A//3vfwHo0qULYGdN3nrrLSB033eHEpNl6dq1a4GfmyxqfHx8gZ/XrFkTgHHjxgFw6623+riEUlImQ2YyIKZ94K+//upax8Rp8eLFgJ2BGTp0KABjx44NSFnFM7/88gtgf9fmzWCf7M8//wTgueeeA2DevHkFrmf+TsybEXGGzp07A/Dggw8C0KFDB5/tMyUlpdT7cgJlzkREREQcJCQzZ+effz5gtw+rXLkyYPfMvPrqqzl06FCJ9tG6dWu35UuXLgXsti7myUycxcRvzpw5XHTRRUD+LOe+ffsA6NWrFwB16tQBYPv27QA0bNjQbf2bbroJUOYsGF566SUAWrRoAcA999wDuGfMDNMe9IUXXgDszFneeIqzXH/99UDhGew//vgDgGuvvdbVU8+MWyanlgEDBgDQuHHjUu/L9NwNlYyZocyZiIiIiIOEROasfv36ADz++OMAtGvXDrB7/bz//vsApKenF7qPyMhIwG6XMnnyZAAqVqzott5HH30EQGZmpi+KLj5mMitmzLLTTz/d9dmePXsAuPvuuwE762LaEY4fPx6A119/HbDHVfJ0FHLxHROT7t27A/ZTctWqVYvddv/+/YDuVaczY9RNnDgRyN8r07wBMe0N9+7dG8DSiT906tQJgG+//dbjbU0Pz9NOOw0I3XbfypyJiIiIOEhIZM52794NwOWXXw7YvetMj8vNmzcD9lgqJ49RZtoZvfrqq4CddTNMLx/T5mXMmDEAZGVl+fgsxBvmKer+++8H4JZbbgHcM2bLli0D7CfzH374ocB9TZ061e33CRMmAMqcBVJMTAxgz5t38803A/a9m5GRAdjtBYti2pVqTEJnM+0HzXexYTJmjz76KKA4hgLzlsvc5+a72ROmndro0aN9Vi4nUuZMRERExEFCInNm2pSYXlkm42HGOzJth8x4VwsXLnT14Bs1ahQATZs2ddunya7ccccdAKxfv94/hZdSMdlSkzkzTDuEtLQ0V1vEwjJmhTG9fCVwzGjweXvEjhgxArB753322WfF7uvMM88E7J7V4kyml2ZeZi5bZcxCx1133QXYY9ht2LCh1PsMtV6ahjJnIiIiIg4SEpkz47vvvgPsmQFMz72HH34YgCFDhgB2tuxkpm3Zk08+Cdg9P/XU5iymV61pp2J6XhomY7Zw4ULAznx6wvQCevPNN70tpnjJtOU04weaJ+xzzjkHgPvuuw+A7OzsYvdlxlIyvvrqK5+VU0rPZDbLlSv4vyEz+0OVKlUA+/tdTl21a9cG7F7XpWH2YdqchxplzkREREQcJKQyZ3mZJ62+ffsC8OWXXwLuo/6bMXPMu/C33347cAUUjz377LMA3HnnnQV+vmDBAgD+85//eH0M86TepEkTt+VmPlbxnwMHDgBw3XXXAbB8+XLAHrOwfPnyALzxxhuA3Qbt559/du3DZMzNXKqmp6cZ71CcwbytKGycKnMvmxibuTdLwvTmNWOjGWYfZm5OCawLLrgAsK//X3/95fW+zN9NqI5zFtKVM8N0FMjb6B8gJycHUMrc6ebPnw/AwIEDC/zcdMk2QzCURseOHQF7MMy8r7zF/0xF2MTiqaeeAuyOAcOHDwfsL+Yff/yR22+/HbBfR1evXh2wmyh8//33gSi6lJD5D/q3334DCv5+BjuOl156qcfH6N27t9vvpnL2wAMPAN4N5SDeMzFetGgRULIhccKVXmuKiIiIOEhIZ86uuuoqAF555RXAHvhu+/btvPzyywAMGzYMsF95mKcz83pFgsu82jCDy+ZNYT/33HOA3UGgNIMDmymfunXrBthZVdMIfc2aNV7vW7xjGvGbe9kMk3PeeecB0KZNGyB3ovu8w92YzIwZQFqcqWfPnoA9jVPLli0Bu2NXaZhOB9WqVQPsAUzNgNQmE9+rV69SH0sKZ+JgvPXWWx7vw3QmSExM9EmZnE6ZMxEREREHCcnMmZmC6bXXXgPsKX5M26EbbrjB9ZS9a9cuAF588UXAfhJ/5513AldgyadZs2aAPUBl3q7XpmOAmRjbm4zZGWecAeBqq9SlSxfA7tqfnJwMwJw5czzet/iWyWTnHQbHtEe68847XW3LDDOFl2lzOm3aNP8WUrzy+++/u/007Q3N9E2lYbJwTzzxBGBPuG2ydHkzOuIfZrBw8z3++eefl2i72rVrk5aWBtj/J+S9z01GLdQocyYiIiLiICGVOTMZDzNRbt6MmRle4eS2KWYaGDMF1NNPP+22/ODBg34utRTk//7v/wCoUKGC23IzVYeZrsnE1hMmY2YGJTbTfBkma2oGMf333389PoYEhhlmwWQ9wR7AtmLFioA9fdvKlSsB2Lp1ayCLKEFk3ow0bNgwyCUJb99++y1gtxl+5plnAHuQcDPcjWGyYSkpKfz000+A3XM7b7vjUB1KQ5kzEREREQcJqcyZ6dFlpv0wUy+Z2rkZW+Vkv/76KwA7d+4EoFGjRgDExcUBypwFy0033QTYT0Wm/dfo0aOB/E9aRalfvz4AN998M2D30GrQoAEA//zzD2CPnWXavKSnp3tbfAkQM6l53bp1Xffq+eefD0CPHj0AeP755wFYtWoVAOeeey4AGRkZgSyqBIAZpNhkXgYNGgTY3wGGuedff/31wBUujJnsl/k/uH///oDdJtC8uTJt0kzWu1GjRtSsWROAHTt2ALn3OtjtTUO1LakyZyIiIiIOEhKZM1PLNu3FDJNlKShjZpgsSnx8vH8KJ0FhnpRr1qzp6rVrMmXG4sWLAXv0ec0Scerp168fkNumaOjQoYCdBTdZkUceeQSwe2+adozKnAWXyW6ZiavNuILeaN68OWC3NzZvUfIyxzBtWk0vfQkMM16lmZnB9KQ2TObs5LcZGzZsAOz2paatsBnjMlQpcyYiIiLiICGRObv44osBuxfe9u3bAVixYkWh25hxb0x2zfT0NGOomLFVJDheffVVwG57ZkaFNpNaf/PNN0Vub56kGzRo4Joj0/TSmzx5MmBPcq8MyqnLzN6QnZ2db2xC871g2qyIs6SmpgL2DC4nTpwA7J595v7M66yzznL1pDbM94Npj5SXyZjNnj0bUMYs2ExsC4txUcwYaSbLZtoSmzaloUKZMxEREREHCYnMWd5RnqdOnQoUnv164oknGDNmDABly5YF7IyZaZ+SnZ3tj6JKCT355JOAPX6VyX6YdiqFtSkxTC/PP//8kwcffBCw2yCZJ3Q59ZlMyZdffuka08r00jT3tHnCNm0MTXsWCS4zRuFtt91W4OelmZnDZMrMGIUmE2/aJcqpz3zHX3vttQD5sqmnOmXORERERBwkJDJnf/31l9vvZjwUw2TWzEjwt99+uytjZuZvmzBhAlC6HkPiO5s2bQLszNnIkSMBuO6669zWMz3zXnjhBbflZoy7uXPn+rWc4gyZmZmu9kSmDYqZIeSrr74CYNiwYYDucafo3LkzYM+P27NnT8Bu/+sJk0Ux3wcTJ04E1LYsFP3yyy9uPxs3bhzM4viNMmciIiIiDhJhOWRiqkOHDlG5cmWvto2MjATsOTPN75s3bwagd+/ebssPHz7sarP0ySefAP5/mk5PT3eNZh7uShPrU4FinSsQcTYZ0pOzLfv37wfsHtnPPvss4J+2hop1Ll/EulWrVgB0794dsHtmX3PNNYVuM3/+fADWrVsHwIIFC0pVhqIo1rmc8v1t2g+a+9u8DfMFJ8RamTMRERERBwmJzJlx9tlnA7BmzRog//hGZqTh7t27u56uA8UJNXGncMqTl78o1rkCEec333wTgL59+/Lpp58Cdtsy027RnxTrXKF+T4NibSjWgaHMmYiIiIiDhERvTcOMAH/aaacFuSQiEghmjCMRkVCizJmIiIiIg6hyJiIiIuIgqpyJiIiIOIgqZyIiIiIO4pjKmUNG9PCbUD8/T4T6tQj18yupcLgO4XCOJREO1yEczrEkwuE6OOEcHVM5y8jICHYR/CrUz88ToX4tQv38SiocrkM4nGNJhMN1CIdzLIlwuA5OOEfHDEKbk5NDWloacXFxREREBLs4PmNZFhkZGdSuXZsyZRxTFw4qxTo8hGqcQbHOS7EOH4p1YDimciYiIiIiDnqtKSIiIiKqnImIiIg4iipnIiIiIg6iypmIiIiIg6hyJiIiIuIgqpyJiIiIOIgqZyIiIiIO8v8A/89ppVHl6KIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run network on data we got before and show predictions\n",
    "output = model2(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0w7iym1T2QY"
   },
   "source": [
    "# IV. Conclusion and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmg5a5HR7b2f"
   },
   "source": [
    "By evaluating Models 1 and 2, we observe a trade-off between accuracy and computational efficiency. Model 1 has a single convolutional layer and a simple structure, and can complete training quickly (29 seconds), but has lower accuracy. Model 2 improves accuracy by adding additional convolutional layers and dropout for regularization, capturing more complex patterns at the cost of increased computation time (2 minutes). This improvement highlights the effectiveness of additional layers and regularization in learning richer features, but also illustrates the longer running time due to the larger number of parameters.\n",
    "\n",
    "To further improve accuracy, I introduce Model 3 (Net3), a deeper architecture with three convolutional layers, batch normalization and increased dropout, as well as the Adam optimizer for faster convergence. These new features will allow Net3 to achieve lower loss and higher accuracy by learning more detailed image features and reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e-hfhdpoPJi",
    "outputId": "ac7e2559-7c2b-4b41-c495-5c26c0659561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000]\tLoss: 2.372719\n",
      "Train Epoch: 1 [640/60000]\tLoss: 1.179105\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 0.532271\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 0.216674\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 0.347728\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 0.195594\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 0.256184\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 0.294808\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 0.215527\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 0.088381\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 0.045382\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 0.076285\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 0.190303\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 0.121894\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 0.144341\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 0.130322\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 0.200358\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 0.017295\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 0.186410\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 0.039413\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 0.022208\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 0.025168\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 0.054632\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 0.127565\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 0.136585\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 0.199752\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 0.077003\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 0.059447\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 0.141350\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 0.144967\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 0.058584\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 0.096233\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 0.044615\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 0.107128\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 0.048648\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 0.115559\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 0.050159\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 0.038668\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 0.036532\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 0.127077\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 0.011248\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 0.080579\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 0.028496\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 0.028482\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 0.135417\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 0.009979\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 0.010336\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 0.042551\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 0.198662\n",
      "Train Epoch: 1 [31360/60000]\tLoss: 0.080131\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 0.024002\n",
      "Train Epoch: 1 [32640/60000]\tLoss: 0.043336\n",
      "Train Epoch: 1 [33280/60000]\tLoss: 0.099123\n",
      "Train Epoch: 1 [33920/60000]\tLoss: 0.143645\n",
      "Train Epoch: 1 [34560/60000]\tLoss: 0.097526\n",
      "Train Epoch: 1 [35200/60000]\tLoss: 0.028325\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 0.126133\n",
      "Train Epoch: 1 [36480/60000]\tLoss: 0.031352\n",
      "Train Epoch: 1 [37120/60000]\tLoss: 0.064533\n",
      "Train Epoch: 1 [37760/60000]\tLoss: 0.073988\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 0.037211\n",
      "Train Epoch: 1 [39040/60000]\tLoss: 0.028184\n",
      "Train Epoch: 1 [39680/60000]\tLoss: 0.130848\n",
      "Train Epoch: 1 [40320/60000]\tLoss: 0.060181\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 0.045817\n",
      "Train Epoch: 1 [41600/60000]\tLoss: 0.171979\n",
      "Train Epoch: 1 [42240/60000]\tLoss: 0.032587\n",
      "Train Epoch: 1 [42880/60000]\tLoss: 0.042469\n",
      "Train Epoch: 1 [43520/60000]\tLoss: 0.056510\n",
      "Train Epoch: 1 [44160/60000]\tLoss: 0.053158\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 0.065182\n",
      "Train Epoch: 1 [45440/60000]\tLoss: 0.053743\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 0.147684\n",
      "Train Epoch: 1 [46720/60000]\tLoss: 0.051917\n",
      "Train Epoch: 1 [47360/60000]\tLoss: 0.025233\n",
      "Train Epoch: 1 [48000/60000]\tLoss: 0.087382\n",
      "Train Epoch: 1 [48640/60000]\tLoss: 0.138721\n",
      "Train Epoch: 1 [49280/60000]\tLoss: 0.004798\n",
      "Train Epoch: 1 [49920/60000]\tLoss: 0.058651\n",
      "Train Epoch: 1 [50560/60000]\tLoss: 0.145063\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 0.010232\n",
      "Train Epoch: 1 [51840/60000]\tLoss: 0.164298\n",
      "Train Epoch: 1 [52480/60000]\tLoss: 0.067070\n",
      "Train Epoch: 1 [53120/60000]\tLoss: 0.126324\n",
      "Train Epoch: 1 [53760/60000]\tLoss: 0.021972\n",
      "Train Epoch: 1 [54400/60000]\tLoss: 0.021859\n",
      "Train Epoch: 1 [55040/60000]\tLoss: 0.057211\n",
      "Train Epoch: 1 [55680/60000]\tLoss: 0.062652\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 0.029268\n",
      "Train Epoch: 1 [56960/60000]\tLoss: 0.076623\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 0.033925\n",
      "Train Epoch: 1 [58240/60000]\tLoss: 0.155518\n",
      "Train Epoch: 1 [58880/60000]\tLoss: 0.012727\n",
      "Train Epoch: 1 [59520/60000]\tLoss: 0.049578\n",
      "Train Epoch: 2 [0/60000]\tLoss: 0.034915\n",
      "Train Epoch: 2 [640/60000]\tLoss: 0.009405\n",
      "Train Epoch: 2 [1280/60000]\tLoss: 0.117253\n",
      "Train Epoch: 2 [1920/60000]\tLoss: 0.070017\n",
      "Train Epoch: 2 [2560/60000]\tLoss: 0.219798\n",
      "Train Epoch: 2 [3200/60000]\tLoss: 0.036967\n",
      "Train Epoch: 2 [3840/60000]\tLoss: 0.017859\n",
      "Train Epoch: 2 [4480/60000]\tLoss: 0.009867\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 0.024121\n",
      "Train Epoch: 2 [5760/60000]\tLoss: 0.031304\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 0.017041\n",
      "Train Epoch: 2 [7040/60000]\tLoss: 0.062663\n",
      "Train Epoch: 2 [7680/60000]\tLoss: 0.055063\n",
      "Train Epoch: 2 [8320/60000]\tLoss: 0.004125\n",
      "Train Epoch: 2 [8960/60000]\tLoss: 0.022617\n",
      "Train Epoch: 2 [9600/60000]\tLoss: 0.078919\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 0.076287\n",
      "Train Epoch: 2 [10880/60000]\tLoss: 0.011964\n",
      "Train Epoch: 2 [11520/60000]\tLoss: 0.006811\n",
      "Train Epoch: 2 [12160/60000]\tLoss: 0.026157\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 0.012003\n",
      "Train Epoch: 2 [13440/60000]\tLoss: 0.012660\n",
      "Train Epoch: 2 [14080/60000]\tLoss: 0.005272\n",
      "Train Epoch: 2 [14720/60000]\tLoss: 0.041132\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 0.019245\n",
      "Train Epoch: 2 [16000/60000]\tLoss: 0.002957\n",
      "Train Epoch: 2 [16640/60000]\tLoss: 0.008681\n",
      "Train Epoch: 2 [17280/60000]\tLoss: 0.024980\n",
      "Train Epoch: 2 [17920/60000]\tLoss: 0.001780\n",
      "Train Epoch: 2 [18560/60000]\tLoss: 0.021723\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.003036\n",
      "Train Epoch: 2 [19840/60000]\tLoss: 0.056846\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 0.006537\n",
      "Train Epoch: 2 [21120/60000]\tLoss: 0.071686\n",
      "Train Epoch: 2 [21760/60000]\tLoss: 0.177966\n",
      "Train Epoch: 2 [22400/60000]\tLoss: 0.008131\n",
      "Train Epoch: 2 [23040/60000]\tLoss: 0.006959\n",
      "Train Epoch: 2 [23680/60000]\tLoss: 0.003388\n",
      "Train Epoch: 2 [24320/60000]\tLoss: 0.018178\n",
      "Train Epoch: 2 [24960/60000]\tLoss: 0.035028\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.039104\n",
      "Train Epoch: 2 [26240/60000]\tLoss: 0.010158\n",
      "Train Epoch: 2 [26880/60000]\tLoss: 0.023410\n",
      "Train Epoch: 2 [27520/60000]\tLoss: 0.040596\n",
      "Train Epoch: 2 [28160/60000]\tLoss: 0.044933\n",
      "Train Epoch: 2 [28800/60000]\tLoss: 0.062270\n",
      "Train Epoch: 2 [29440/60000]\tLoss: 0.014064\n",
      "Train Epoch: 2 [30080/60000]\tLoss: 0.036164\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 0.044214\n",
      "Train Epoch: 2 [31360/60000]\tLoss: 0.010240\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.034469\n",
      "Train Epoch: 2 [32640/60000]\tLoss: 0.195917\n",
      "Train Epoch: 2 [33280/60000]\tLoss: 0.244849\n",
      "Train Epoch: 2 [33920/60000]\tLoss: 0.046542\n",
      "Train Epoch: 2 [34560/60000]\tLoss: 0.017608\n",
      "Train Epoch: 2 [35200/60000]\tLoss: 0.019877\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 0.085852\n",
      "Train Epoch: 2 [36480/60000]\tLoss: 0.009159\n",
      "Train Epoch: 2 [37120/60000]\tLoss: 0.050949\n",
      "Train Epoch: 2 [37760/60000]\tLoss: 0.014977\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 0.023832\n",
      "Train Epoch: 2 [39040/60000]\tLoss: 0.014412\n",
      "Train Epoch: 2 [39680/60000]\tLoss: 0.026674\n",
      "Train Epoch: 2 [40320/60000]\tLoss: 0.038682\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 0.048619\n",
      "Train Epoch: 2 [41600/60000]\tLoss: 0.036041\n",
      "Train Epoch: 2 [42240/60000]\tLoss: 0.034322\n",
      "Train Epoch: 2 [42880/60000]\tLoss: 0.072303\n",
      "Train Epoch: 2 [43520/60000]\tLoss: 0.045430\n",
      "Train Epoch: 2 [44160/60000]\tLoss: 0.029516\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 0.029564\n",
      "Train Epoch: 2 [45440/60000]\tLoss: 0.083058\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 0.016325\n",
      "Train Epoch: 2 [46720/60000]\tLoss: 0.068159\n",
      "Train Epoch: 2 [47360/60000]\tLoss: 0.026750\n",
      "Train Epoch: 2 [48000/60000]\tLoss: 0.004964\n",
      "Train Epoch: 2 [48640/60000]\tLoss: 0.039410\n",
      "Train Epoch: 2 [49280/60000]\tLoss: 0.024030\n",
      "Train Epoch: 2 [49920/60000]\tLoss: 0.016252\n",
      "Train Epoch: 2 [50560/60000]\tLoss: 0.004615\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.059751\n",
      "Train Epoch: 2 [51840/60000]\tLoss: 0.033405\n",
      "Train Epoch: 2 [52480/60000]\tLoss: 0.038729\n",
      "Train Epoch: 2 [53120/60000]\tLoss: 0.055352\n",
      "Train Epoch: 2 [53760/60000]\tLoss: 0.008407\n",
      "Train Epoch: 2 [54400/60000]\tLoss: 0.011375\n",
      "Train Epoch: 2 [55040/60000]\tLoss: 0.050838\n",
      "Train Epoch: 2 [55680/60000]\tLoss: 0.036486\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 0.157459\n",
      "Train Epoch: 2 [56960/60000]\tLoss: 0.003699\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 0.050133\n",
      "Train Epoch: 2 [58240/60000]\tLoss: 0.005069\n",
      "Train Epoch: 2 [58880/60000]\tLoss: 0.002771\n",
      "Train Epoch: 2 [59520/60000]\tLoss: 0.010438\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.005418\n",
      "Train Epoch: 3 [640/60000]\tLoss: 0.050292\n",
      "Train Epoch: 3 [1280/60000]\tLoss: 0.005638\n",
      "Train Epoch: 3 [1920/60000]\tLoss: 0.014696\n",
      "Train Epoch: 3 [2560/60000]\tLoss: 0.004481\n",
      "Train Epoch: 3 [3200/60000]\tLoss: 0.006696\n",
      "Train Epoch: 3 [3840/60000]\tLoss: 0.011173\n",
      "Train Epoch: 3 [4480/60000]\tLoss: 0.006764\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.003540\n",
      "Train Epoch: 3 [5760/60000]\tLoss: 0.118859\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 0.007752\n",
      "Train Epoch: 3 [7040/60000]\tLoss: 0.012873\n",
      "Train Epoch: 3 [7680/60000]\tLoss: 0.009307\n",
      "Train Epoch: 3 [8320/60000]\tLoss: 0.010655\n",
      "Train Epoch: 3 [8960/60000]\tLoss: 0.001622\n",
      "Train Epoch: 3 [9600/60000]\tLoss: 0.007646\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.004265\n",
      "Train Epoch: 3 [10880/60000]\tLoss: 0.002925\n",
      "Train Epoch: 3 [11520/60000]\tLoss: 0.001560\n",
      "Train Epoch: 3 [12160/60000]\tLoss: 0.002502\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.022154\n",
      "Train Epoch: 3 [13440/60000]\tLoss: 0.014437\n",
      "Train Epoch: 3 [14080/60000]\tLoss: 0.039811\n",
      "Train Epoch: 3 [14720/60000]\tLoss: 0.056759\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 0.024675\n",
      "Train Epoch: 3 [16000/60000]\tLoss: 0.013787\n",
      "Train Epoch: 3 [16640/60000]\tLoss: 0.015645\n",
      "Train Epoch: 3 [17280/60000]\tLoss: 0.034465\n",
      "Train Epoch: 3 [17920/60000]\tLoss: 0.005533\n",
      "Train Epoch: 3 [18560/60000]\tLoss: 0.068170\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 0.002413\n",
      "Train Epoch: 3 [19840/60000]\tLoss: 0.095699\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.003199\n",
      "Train Epoch: 3 [21120/60000]\tLoss: 0.004336\n",
      "Train Epoch: 3 [21760/60000]\tLoss: 0.006214\n",
      "Train Epoch: 3 [22400/60000]\tLoss: 0.044927\n",
      "Train Epoch: 3 [23040/60000]\tLoss: 0.045170\n",
      "Train Epoch: 3 [23680/60000]\tLoss: 0.053370\n",
      "Train Epoch: 3 [24320/60000]\tLoss: 0.054446\n",
      "Train Epoch: 3 [24960/60000]\tLoss: 0.016306\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.006673\n",
      "Train Epoch: 3 [26240/60000]\tLoss: 0.024982\n",
      "Train Epoch: 3 [26880/60000]\tLoss: 0.031194\n",
      "Train Epoch: 3 [27520/60000]\tLoss: 0.049121\n",
      "Train Epoch: 3 [28160/60000]\tLoss: 0.011826\n",
      "Train Epoch: 3 [28800/60000]\tLoss: 0.092462\n",
      "Train Epoch: 3 [29440/60000]\tLoss: 0.006209\n",
      "Train Epoch: 3 [30080/60000]\tLoss: 0.019036\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 0.008535\n",
      "Train Epoch: 3 [31360/60000]\tLoss: 0.012881\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 0.115938\n",
      "Train Epoch: 3 [32640/60000]\tLoss: 0.013936\n",
      "Train Epoch: 3 [33280/60000]\tLoss: 0.014865\n",
      "Train Epoch: 3 [33920/60000]\tLoss: 0.003132\n",
      "Train Epoch: 3 [34560/60000]\tLoss: 0.126811\n",
      "Train Epoch: 3 [35200/60000]\tLoss: 0.021954\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 0.015059\n",
      "Train Epoch: 3 [36480/60000]\tLoss: 0.074225\n",
      "Train Epoch: 3 [37120/60000]\tLoss: 0.019242\n",
      "Train Epoch: 3 [37760/60000]\tLoss: 0.011064\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.136256\n",
      "Train Epoch: 3 [39040/60000]\tLoss: 0.003903\n",
      "Train Epoch: 3 [39680/60000]\tLoss: 0.078894\n",
      "Train Epoch: 3 [40320/60000]\tLoss: 0.010849\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.003031\n",
      "Train Epoch: 3 [41600/60000]\tLoss: 0.011964\n",
      "Train Epoch: 3 [42240/60000]\tLoss: 0.023832\n",
      "Train Epoch: 3 [42880/60000]\tLoss: 0.002078\n",
      "Train Epoch: 3 [43520/60000]\tLoss: 0.026924\n",
      "Train Epoch: 3 [44160/60000]\tLoss: 0.108709\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 0.051130\n",
      "Train Epoch: 3 [45440/60000]\tLoss: 0.052174\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 0.028483\n",
      "Train Epoch: 3 [46720/60000]\tLoss: 0.038114\n",
      "Train Epoch: 3 [47360/60000]\tLoss: 0.072269\n",
      "Train Epoch: 3 [48000/60000]\tLoss: 0.078907\n",
      "Train Epoch: 3 [48640/60000]\tLoss: 0.049759\n",
      "Train Epoch: 3 [49280/60000]\tLoss: 0.006165\n",
      "Train Epoch: 3 [49920/60000]\tLoss: 0.001245\n",
      "Train Epoch: 3 [50560/60000]\tLoss: 0.014882\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.002885\n",
      "Train Epoch: 3 [51840/60000]\tLoss: 0.004832\n",
      "Train Epoch: 3 [52480/60000]\tLoss: 0.037248\n",
      "Train Epoch: 3 [53120/60000]\tLoss: 0.017689\n",
      "Train Epoch: 3 [53760/60000]\tLoss: 0.002126\n",
      "Train Epoch: 3 [54400/60000]\tLoss: 0.026133\n",
      "Train Epoch: 3 [55040/60000]\tLoss: 0.028839\n",
      "Train Epoch: 3 [55680/60000]\tLoss: 0.137455\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.016727\n",
      "Train Epoch: 3 [56960/60000]\tLoss: 0.045080\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.064376\n",
      "Train Epoch: 3 [58240/60000]\tLoss: 0.010036\n",
      "Train Epoch: 3 [58880/60000]\tLoss: 0.007982\n",
      "Train Epoch: 3 [59520/60000]\tLoss: 0.007871\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.014615\n",
      "Train Epoch: 4 [640/60000]\tLoss: 0.003019\n",
      "Train Epoch: 4 [1280/60000]\tLoss: 0.005391\n",
      "Train Epoch: 4 [1920/60000]\tLoss: 0.005906\n",
      "Train Epoch: 4 [2560/60000]\tLoss: 0.007198\n",
      "Train Epoch: 4 [3200/60000]\tLoss: 0.019426\n",
      "Train Epoch: 4 [3840/60000]\tLoss: 0.001837\n",
      "Train Epoch: 4 [4480/60000]\tLoss: 0.009998\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.074136\n",
      "Train Epoch: 4 [5760/60000]\tLoss: 0.008654\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 0.022293\n",
      "Train Epoch: 4 [7040/60000]\tLoss: 0.021455\n",
      "Train Epoch: 4 [7680/60000]\tLoss: 0.003787\n",
      "Train Epoch: 4 [8320/60000]\tLoss: 0.014001\n",
      "Train Epoch: 4 [8960/60000]\tLoss: 0.047663\n",
      "Train Epoch: 4 [9600/60000]\tLoss: 0.015663\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.006008\n",
      "Train Epoch: 4 [10880/60000]\tLoss: 0.003393\n",
      "Train Epoch: 4 [11520/60000]\tLoss: 0.061034\n",
      "Train Epoch: 4 [12160/60000]\tLoss: 0.000297\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.091478\n",
      "Train Epoch: 4 [13440/60000]\tLoss: 0.003717\n",
      "Train Epoch: 4 [14080/60000]\tLoss: 0.036987\n",
      "Train Epoch: 4 [14720/60000]\tLoss: 0.013036\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 0.030481\n",
      "Train Epoch: 4 [16000/60000]\tLoss: 0.002128\n",
      "Train Epoch: 4 [16640/60000]\tLoss: 0.012680\n",
      "Train Epoch: 4 [17280/60000]\tLoss: 0.009718\n",
      "Train Epoch: 4 [17920/60000]\tLoss: 0.006055\n",
      "Train Epoch: 4 [18560/60000]\tLoss: 0.001309\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.035825\n",
      "Train Epoch: 4 [19840/60000]\tLoss: 0.034073\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.007249\n",
      "Train Epoch: 4 [21120/60000]\tLoss: 0.063893\n",
      "Train Epoch: 4 [21760/60000]\tLoss: 0.006019\n",
      "Train Epoch: 4 [22400/60000]\tLoss: 0.036043\n",
      "Train Epoch: 4 [23040/60000]\tLoss: 0.001506\n",
      "Train Epoch: 4 [23680/60000]\tLoss: 0.083737\n",
      "Train Epoch: 4 [24320/60000]\tLoss: 0.005298\n",
      "Train Epoch: 4 [24960/60000]\tLoss: 0.006336\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.027320\n",
      "Train Epoch: 4 [26240/60000]\tLoss: 0.005065\n",
      "Train Epoch: 4 [26880/60000]\tLoss: 0.003668\n",
      "Train Epoch: 4 [27520/60000]\tLoss: 0.001199\n",
      "Train Epoch: 4 [28160/60000]\tLoss: 0.023591\n",
      "Train Epoch: 4 [28800/60000]\tLoss: 0.002129\n",
      "Train Epoch: 4 [29440/60000]\tLoss: 0.003245\n",
      "Train Epoch: 4 [30080/60000]\tLoss: 0.005864\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 0.015145\n",
      "Train Epoch: 4 [31360/60000]\tLoss: 0.009226\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.045346\n",
      "Train Epoch: 4 [32640/60000]\tLoss: 0.013044\n",
      "Train Epoch: 4 [33280/60000]\tLoss: 0.011006\n",
      "Train Epoch: 4 [33920/60000]\tLoss: 0.003629\n",
      "Train Epoch: 4 [34560/60000]\tLoss: 0.007247\n",
      "Train Epoch: 4 [35200/60000]\tLoss: 0.048060\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.001283\n",
      "Train Epoch: 4 [36480/60000]\tLoss: 0.001077\n",
      "Train Epoch: 4 [37120/60000]\tLoss: 0.152350\n",
      "Train Epoch: 4 [37760/60000]\tLoss: 0.008030\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 0.060474\n",
      "Train Epoch: 4 [39040/60000]\tLoss: 0.011481\n",
      "Train Epoch: 4 [39680/60000]\tLoss: 0.015142\n",
      "Train Epoch: 4 [40320/60000]\tLoss: 0.024134\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.000314\n",
      "Train Epoch: 4 [41600/60000]\tLoss: 0.009307\n",
      "Train Epoch: 4 [42240/60000]\tLoss: 0.004829\n",
      "Train Epoch: 4 [42880/60000]\tLoss: 0.023202\n",
      "Train Epoch: 4 [43520/60000]\tLoss: 0.025107\n",
      "Train Epoch: 4 [44160/60000]\tLoss: 0.006078\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 0.062952\n",
      "Train Epoch: 4 [45440/60000]\tLoss: 0.059405\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.089449\n",
      "Train Epoch: 4 [46720/60000]\tLoss: 0.068493\n",
      "Train Epoch: 4 [47360/60000]\tLoss: 0.006973\n",
      "Train Epoch: 4 [48000/60000]\tLoss: 0.008218\n",
      "Train Epoch: 4 [48640/60000]\tLoss: 0.022837\n",
      "Train Epoch: 4 [49280/60000]\tLoss: 0.050601\n",
      "Train Epoch: 4 [49920/60000]\tLoss: 0.012206\n",
      "Train Epoch: 4 [50560/60000]\tLoss: 0.084660\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.000419\n",
      "Train Epoch: 4 [51840/60000]\tLoss: 0.033242\n",
      "Train Epoch: 4 [52480/60000]\tLoss: 0.000724\n",
      "Train Epoch: 4 [53120/60000]\tLoss: 0.000747\n",
      "Train Epoch: 4 [53760/60000]\tLoss: 0.002795\n",
      "Train Epoch: 4 [54400/60000]\tLoss: 0.024673\n",
      "Train Epoch: 4 [55040/60000]\tLoss: 0.146915\n",
      "Train Epoch: 4 [55680/60000]\tLoss: 0.002583\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.010711\n",
      "Train Epoch: 4 [56960/60000]\tLoss: 0.003087\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.003530\n",
      "Train Epoch: 4 [58240/60000]\tLoss: 0.003384\n",
      "Train Epoch: 4 [58880/60000]\tLoss: 0.041228\n",
      "Train Epoch: 4 [59520/60000]\tLoss: 0.001684\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.002718\n",
      "Train Epoch: 5 [640/60000]\tLoss: 0.005386\n",
      "Train Epoch: 5 [1280/60000]\tLoss: 0.017080\n",
      "Train Epoch: 5 [1920/60000]\tLoss: 0.008676\n",
      "Train Epoch: 5 [2560/60000]\tLoss: 0.000667\n",
      "Train Epoch: 5 [3200/60000]\tLoss: 0.004592\n",
      "Train Epoch: 5 [3840/60000]\tLoss: 0.010292\n",
      "Train Epoch: 5 [4480/60000]\tLoss: 0.002786\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.004831\n",
      "Train Epoch: 5 [5760/60000]\tLoss: 0.059767\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.012493\n",
      "Train Epoch: 5 [7040/60000]\tLoss: 0.013456\n",
      "Train Epoch: 5 [7680/60000]\tLoss: 0.007127\n",
      "Train Epoch: 5 [8320/60000]\tLoss: 0.011838\n",
      "Train Epoch: 5 [8960/60000]\tLoss: 0.035062\n",
      "Train Epoch: 5 [9600/60000]\tLoss: 0.019784\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 0.001627\n",
      "Train Epoch: 5 [10880/60000]\tLoss: 0.003231\n",
      "Train Epoch: 5 [11520/60000]\tLoss: 0.024176\n",
      "Train Epoch: 5 [12160/60000]\tLoss: 0.006672\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 0.002302\n",
      "Train Epoch: 5 [13440/60000]\tLoss: 0.077124\n",
      "Train Epoch: 5 [14080/60000]\tLoss: 0.003754\n",
      "Train Epoch: 5 [14720/60000]\tLoss: 0.002969\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 0.003141\n",
      "Train Epoch: 5 [16000/60000]\tLoss: 0.020054\n",
      "Train Epoch: 5 [16640/60000]\tLoss: 0.000894\n",
      "Train Epoch: 5 [17280/60000]\tLoss: 0.017020\n",
      "Train Epoch: 5 [17920/60000]\tLoss: 0.011192\n",
      "Train Epoch: 5 [18560/60000]\tLoss: 0.042413\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.006943\n",
      "Train Epoch: 5 [19840/60000]\tLoss: 0.002308\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.000376\n",
      "Train Epoch: 5 [21120/60000]\tLoss: 0.000728\n",
      "Train Epoch: 5 [21760/60000]\tLoss: 0.002702\n",
      "Train Epoch: 5 [22400/60000]\tLoss: 0.071183\n",
      "Train Epoch: 5 [23040/60000]\tLoss: 0.009200\n",
      "Train Epoch: 5 [23680/60000]\tLoss: 0.007431\n",
      "Train Epoch: 5 [24320/60000]\tLoss: 0.020031\n",
      "Train Epoch: 5 [24960/60000]\tLoss: 0.030685\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.002762\n",
      "Train Epoch: 5 [26240/60000]\tLoss: 0.001182\n",
      "Train Epoch: 5 [26880/60000]\tLoss: 0.027027\n",
      "Train Epoch: 5 [27520/60000]\tLoss: 0.001528\n",
      "Train Epoch: 5 [28160/60000]\tLoss: 0.015150\n",
      "Train Epoch: 5 [28800/60000]\tLoss: 0.004675\n",
      "Train Epoch: 5 [29440/60000]\tLoss: 0.003800\n",
      "Train Epoch: 5 [30080/60000]\tLoss: 0.124298\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 0.016180\n",
      "Train Epoch: 5 [31360/60000]\tLoss: 0.030987\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.005656\n",
      "Train Epoch: 5 [32640/60000]\tLoss: 0.025467\n",
      "Train Epoch: 5 [33280/60000]\tLoss: 0.012235\n",
      "Train Epoch: 5 [33920/60000]\tLoss: 0.010298\n",
      "Train Epoch: 5 [34560/60000]\tLoss: 0.007319\n",
      "Train Epoch: 5 [35200/60000]\tLoss: 0.002263\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 0.011746\n",
      "Train Epoch: 5 [36480/60000]\tLoss: 0.000222\n",
      "Train Epoch: 5 [37120/60000]\tLoss: 0.027999\n",
      "Train Epoch: 5 [37760/60000]\tLoss: 0.010361\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 0.004115\n",
      "Train Epoch: 5 [39040/60000]\tLoss: 0.004664\n",
      "Train Epoch: 5 [39680/60000]\tLoss: 0.011980\n",
      "Train Epoch: 5 [40320/60000]\tLoss: 0.007865\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 0.002063\n",
      "Train Epoch: 5 [41600/60000]\tLoss: 0.004023\n",
      "Train Epoch: 5 [42240/60000]\tLoss: 0.051019\n",
      "Train Epoch: 5 [42880/60000]\tLoss: 0.000830\n",
      "Train Epoch: 5 [43520/60000]\tLoss: 0.070477\n",
      "Train Epoch: 5 [44160/60000]\tLoss: 0.003202\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 0.001252\n",
      "Train Epoch: 5 [45440/60000]\tLoss: 0.001155\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 0.000391\n",
      "Train Epoch: 5 [46720/60000]\tLoss: 0.001996\n",
      "Train Epoch: 5 [47360/60000]\tLoss: 0.003580\n",
      "Train Epoch: 5 [48000/60000]\tLoss: 0.001369\n",
      "Train Epoch: 5 [48640/60000]\tLoss: 0.008319\n",
      "Train Epoch: 5 [49280/60000]\tLoss: 0.039652\n",
      "Train Epoch: 5 [49920/60000]\tLoss: 0.002897\n",
      "Train Epoch: 5 [50560/60000]\tLoss: 0.016598\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.003729\n",
      "Train Epoch: 5 [51840/60000]\tLoss: 0.003741\n",
      "Train Epoch: 5 [52480/60000]\tLoss: 0.001369\n",
      "Train Epoch: 5 [53120/60000]\tLoss: 0.133820\n",
      "Train Epoch: 5 [53760/60000]\tLoss: 0.000729\n",
      "Train Epoch: 5 [54400/60000]\tLoss: 0.000730\n",
      "Train Epoch: 5 [55040/60000]\tLoss: 0.018549\n",
      "Train Epoch: 5 [55680/60000]\tLoss: 0.003356\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 0.015147\n",
      "Train Epoch: 5 [56960/60000]\tLoss: 0.048723\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.023832\n",
      "Train Epoch: 5 [58240/60000]\tLoss: 0.001289\n",
      "Train Epoch: 5 [58880/60000]\tLoss: 0.044701\n",
      "Train Epoch: 5 [59520/60000]\tLoss: 0.000341\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.007798\n",
      "Train Epoch: 6 [640/60000]\tLoss: 0.136116\n",
      "Train Epoch: 6 [1280/60000]\tLoss: 0.001679\n",
      "Train Epoch: 6 [1920/60000]\tLoss: 0.065196\n",
      "Train Epoch: 6 [2560/60000]\tLoss: 0.008678\n",
      "Train Epoch: 6 [3200/60000]\tLoss: 0.030178\n",
      "Train Epoch: 6 [3840/60000]\tLoss: 0.001943\n",
      "Train Epoch: 6 [4480/60000]\tLoss: 0.002710\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 0.002968\n",
      "Train Epoch: 6 [5760/60000]\tLoss: 0.003515\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 0.002445\n",
      "Train Epoch: 6 [7040/60000]\tLoss: 0.002745\n",
      "Train Epoch: 6 [7680/60000]\tLoss: 0.041078\n",
      "Train Epoch: 6 [8320/60000]\tLoss: 0.004515\n",
      "Train Epoch: 6 [8960/60000]\tLoss: 0.001474\n",
      "Train Epoch: 6 [9600/60000]\tLoss: 0.010687\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 0.045965\n",
      "Train Epoch: 6 [10880/60000]\tLoss: 0.007101\n",
      "Train Epoch: 6 [11520/60000]\tLoss: 0.001079\n",
      "Train Epoch: 6 [12160/60000]\tLoss: 0.001076\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.017021\n",
      "Train Epoch: 6 [13440/60000]\tLoss: 0.002210\n",
      "Train Epoch: 6 [14080/60000]\tLoss: 0.001953\n",
      "Train Epoch: 6 [14720/60000]\tLoss: 0.000419\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.003774\n",
      "Train Epoch: 6 [16000/60000]\tLoss: 0.006968\n",
      "Train Epoch: 6 [16640/60000]\tLoss: 0.027310\n",
      "Train Epoch: 6 [17280/60000]\tLoss: 0.004914\n",
      "Train Epoch: 6 [17920/60000]\tLoss: 0.005701\n",
      "Train Epoch: 6 [18560/60000]\tLoss: 0.001574\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 0.006922\n",
      "Train Epoch: 6 [19840/60000]\tLoss: 0.000158\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 0.143858\n",
      "Train Epoch: 6 [21120/60000]\tLoss: 0.018960\n",
      "Train Epoch: 6 [21760/60000]\tLoss: 0.001184\n",
      "Train Epoch: 6 [22400/60000]\tLoss: 0.000299\n",
      "Train Epoch: 6 [23040/60000]\tLoss: 0.002721\n",
      "Train Epoch: 6 [23680/60000]\tLoss: 0.007924\n",
      "Train Epoch: 6 [24320/60000]\tLoss: 0.000078\n",
      "Train Epoch: 6 [24960/60000]\tLoss: 0.002945\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.015644\n",
      "Train Epoch: 6 [26240/60000]\tLoss: 0.004893\n",
      "Train Epoch: 6 [26880/60000]\tLoss: 0.001772\n",
      "Train Epoch: 6 [27520/60000]\tLoss: 0.019968\n",
      "Train Epoch: 6 [28160/60000]\tLoss: 0.000703\n",
      "Train Epoch: 6 [28800/60000]\tLoss: 0.028220\n",
      "Train Epoch: 6 [29440/60000]\tLoss: 0.005900\n",
      "Train Epoch: 6 [30080/60000]\tLoss: 0.067145\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.028938\n",
      "Train Epoch: 6 [31360/60000]\tLoss: 0.004727\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.095910\n",
      "Train Epoch: 6 [32640/60000]\tLoss: 0.012077\n",
      "Train Epoch: 6 [33280/60000]\tLoss: 0.001229\n",
      "Train Epoch: 6 [33920/60000]\tLoss: 0.000587\n",
      "Train Epoch: 6 [34560/60000]\tLoss: 0.002694\n",
      "Train Epoch: 6 [35200/60000]\tLoss: 0.000210\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.028628\n",
      "Train Epoch: 6 [36480/60000]\tLoss: 0.007233\n",
      "Train Epoch: 6 [37120/60000]\tLoss: 0.000618\n",
      "Train Epoch: 6 [37760/60000]\tLoss: 0.001213\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 0.012076\n",
      "Train Epoch: 6 [39040/60000]\tLoss: 0.000348\n",
      "Train Epoch: 6 [39680/60000]\tLoss: 0.050379\n",
      "Train Epoch: 6 [40320/60000]\tLoss: 0.002350\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.033996\n",
      "Train Epoch: 6 [41600/60000]\tLoss: 0.002613\n",
      "Train Epoch: 6 [42240/60000]\tLoss: 0.001529\n",
      "Train Epoch: 6 [42880/60000]\tLoss: 0.001860\n",
      "Train Epoch: 6 [43520/60000]\tLoss: 0.000854\n",
      "Train Epoch: 6 [44160/60000]\tLoss: 0.005247\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.001720\n",
      "Train Epoch: 6 [45440/60000]\tLoss: 0.020329\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.000310\n",
      "Train Epoch: 6 [46720/60000]\tLoss: 0.000208\n",
      "Train Epoch: 6 [47360/60000]\tLoss: 0.000800\n",
      "Train Epoch: 6 [48000/60000]\tLoss: 0.014713\n",
      "Train Epoch: 6 [48640/60000]\tLoss: 0.005528\n",
      "Train Epoch: 6 [49280/60000]\tLoss: 0.001429\n",
      "Train Epoch: 6 [49920/60000]\tLoss: 0.002823\n",
      "Train Epoch: 6 [50560/60000]\tLoss: 0.000283\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.004272\n",
      "Train Epoch: 6 [51840/60000]\tLoss: 0.032360\n",
      "Train Epoch: 6 [52480/60000]\tLoss: 0.001017\n",
      "Train Epoch: 6 [53120/60000]\tLoss: 0.002214\n",
      "Train Epoch: 6 [53760/60000]\tLoss: 0.006460\n",
      "Train Epoch: 6 [54400/60000]\tLoss: 0.000415\n",
      "Train Epoch: 6 [55040/60000]\tLoss: 0.010103\n",
      "Train Epoch: 6 [55680/60000]\tLoss: 0.047791\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 0.019797\n",
      "Train Epoch: 6 [56960/60000]\tLoss: 0.002068\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.030121\n",
      "Train Epoch: 6 [58240/60000]\tLoss: 0.004341\n",
      "Train Epoch: 6 [58880/60000]\tLoss: 0.021046\n",
      "Train Epoch: 6 [59520/60000]\tLoss: 0.003351\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.009806\n",
      "Train Epoch: 7 [640/60000]\tLoss: 0.006419\n",
      "Train Epoch: 7 [1280/60000]\tLoss: 0.068290\n",
      "Train Epoch: 7 [1920/60000]\tLoss: 0.003248\n",
      "Train Epoch: 7 [2560/60000]\tLoss: 0.046411\n",
      "Train Epoch: 7 [3200/60000]\tLoss: 0.012019\n",
      "Train Epoch: 7 [3840/60000]\tLoss: 0.007142\n",
      "Train Epoch: 7 [4480/60000]\tLoss: 0.000637\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.074391\n",
      "Train Epoch: 7 [5760/60000]\tLoss: 0.037463\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 0.004509\n",
      "Train Epoch: 7 [7040/60000]\tLoss: 0.000232\n",
      "Train Epoch: 7 [7680/60000]\tLoss: 0.000718\n",
      "Train Epoch: 7 [8320/60000]\tLoss: 0.000368\n",
      "Train Epoch: 7 [8960/60000]\tLoss: 0.036418\n",
      "Train Epoch: 7 [9600/60000]\tLoss: 0.001162\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.052228\n",
      "Train Epoch: 7 [10880/60000]\tLoss: 0.003841\n",
      "Train Epoch: 7 [11520/60000]\tLoss: 0.001073\n",
      "Train Epoch: 7 [12160/60000]\tLoss: 0.012221\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 0.015616\n",
      "Train Epoch: 7 [13440/60000]\tLoss: 0.000545\n",
      "Train Epoch: 7 [14080/60000]\tLoss: 0.016122\n",
      "Train Epoch: 7 [14720/60000]\tLoss: 0.000720\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 0.002496\n",
      "Train Epoch: 7 [16000/60000]\tLoss: 0.000703\n",
      "Train Epoch: 7 [16640/60000]\tLoss: 0.004404\n",
      "Train Epoch: 7 [17280/60000]\tLoss: 0.002616\n",
      "Train Epoch: 7 [17920/60000]\tLoss: 0.000044\n",
      "Train Epoch: 7 [18560/60000]\tLoss: 0.000301\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.035449\n",
      "Train Epoch: 7 [19840/60000]\tLoss: 0.017543\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 0.040817\n",
      "Train Epoch: 7 [21120/60000]\tLoss: 0.001358\n",
      "Train Epoch: 7 [21760/60000]\tLoss: 0.001572\n",
      "Train Epoch: 7 [22400/60000]\tLoss: 0.024540\n",
      "Train Epoch: 7 [23040/60000]\tLoss: 0.006338\n",
      "Train Epoch: 7 [23680/60000]\tLoss: 0.000123\n",
      "Train Epoch: 7 [24320/60000]\tLoss: 0.000335\n",
      "Train Epoch: 7 [24960/60000]\tLoss: 0.018884\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.140344\n",
      "Train Epoch: 7 [26240/60000]\tLoss: 0.017572\n",
      "Train Epoch: 7 [26880/60000]\tLoss: 0.010980\n",
      "Train Epoch: 7 [27520/60000]\tLoss: 0.010713\n",
      "Train Epoch: 7 [28160/60000]\tLoss: 0.065341\n",
      "Train Epoch: 7 [28800/60000]\tLoss: 0.000429\n",
      "Train Epoch: 7 [29440/60000]\tLoss: 0.011415\n",
      "Train Epoch: 7 [30080/60000]\tLoss: 0.001739\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 0.014410\n",
      "Train Epoch: 7 [31360/60000]\tLoss: 0.000141\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.002026\n",
      "Train Epoch: 7 [32640/60000]\tLoss: 0.001580\n",
      "Train Epoch: 7 [33280/60000]\tLoss: 0.028676\n",
      "Train Epoch: 7 [33920/60000]\tLoss: 0.005596\n",
      "Train Epoch: 7 [34560/60000]\tLoss: 0.003172\n",
      "Train Epoch: 7 [35200/60000]\tLoss: 0.001927\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.002614\n",
      "Train Epoch: 7 [36480/60000]\tLoss: 0.037865\n",
      "Train Epoch: 7 [37120/60000]\tLoss: 0.020793\n",
      "Train Epoch: 7 [37760/60000]\tLoss: 0.000744\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 0.001548\n",
      "Train Epoch: 7 [39040/60000]\tLoss: 0.005842\n",
      "Train Epoch: 7 [39680/60000]\tLoss: 0.009268\n",
      "Train Epoch: 7 [40320/60000]\tLoss: 0.003400\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.000789\n",
      "Train Epoch: 7 [41600/60000]\tLoss: 0.000727\n",
      "Train Epoch: 7 [42240/60000]\tLoss: 0.000753\n",
      "Train Epoch: 7 [42880/60000]\tLoss: 0.003228\n",
      "Train Epoch: 7 [43520/60000]\tLoss: 0.005627\n",
      "Train Epoch: 7 [44160/60000]\tLoss: 0.081213\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.003277\n",
      "Train Epoch: 7 [45440/60000]\tLoss: 0.101625\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.001473\n",
      "Train Epoch: 7 [46720/60000]\tLoss: 0.007085\n",
      "Train Epoch: 7 [47360/60000]\tLoss: 0.000869\n",
      "Train Epoch: 7 [48000/60000]\tLoss: 0.001999\n",
      "Train Epoch: 7 [48640/60000]\tLoss: 0.001252\n",
      "Train Epoch: 7 [49280/60000]\tLoss: 0.000956\n",
      "Train Epoch: 7 [49920/60000]\tLoss: 0.002181\n",
      "Train Epoch: 7 [50560/60000]\tLoss: 0.000352\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.001188\n",
      "Train Epoch: 7 [51840/60000]\tLoss: 0.003050\n",
      "Train Epoch: 7 [52480/60000]\tLoss: 0.001840\n",
      "Train Epoch: 7 [53120/60000]\tLoss: 0.001898\n",
      "Train Epoch: 7 [53760/60000]\tLoss: 0.002417\n",
      "Train Epoch: 7 [54400/60000]\tLoss: 0.000942\n",
      "Train Epoch: 7 [55040/60000]\tLoss: 0.019676\n",
      "Train Epoch: 7 [55680/60000]\tLoss: 0.003791\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 0.017310\n",
      "Train Epoch: 7 [56960/60000]\tLoss: 0.003577\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 0.001823\n",
      "Train Epoch: 7 [58240/60000]\tLoss: 0.001108\n",
      "Train Epoch: 7 [58880/60000]\tLoss: 0.123063\n",
      "Train Epoch: 7 [59520/60000]\tLoss: 0.010078\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.003632\n",
      "Train Epoch: 8 [640/60000]\tLoss: 0.000447\n",
      "Train Epoch: 8 [1280/60000]\tLoss: 0.002308\n",
      "Train Epoch: 8 [1920/60000]\tLoss: 0.004930\n",
      "Train Epoch: 8 [2560/60000]\tLoss: 0.004241\n",
      "Train Epoch: 8 [3200/60000]\tLoss: 0.065101\n",
      "Train Epoch: 8 [3840/60000]\tLoss: 0.003967\n",
      "Train Epoch: 8 [4480/60000]\tLoss: 0.000940\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.002587\n",
      "Train Epoch: 8 [5760/60000]\tLoss: 0.001874\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.005890\n",
      "Train Epoch: 8 [7040/60000]\tLoss: 0.001107\n",
      "Train Epoch: 8 [7680/60000]\tLoss: 0.007465\n",
      "Train Epoch: 8 [8320/60000]\tLoss: 0.051199\n",
      "Train Epoch: 8 [8960/60000]\tLoss: 0.000462\n",
      "Train Epoch: 8 [9600/60000]\tLoss: 0.000385\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 0.007061\n",
      "Train Epoch: 8 [10880/60000]\tLoss: 0.003596\n",
      "Train Epoch: 8 [11520/60000]\tLoss: 0.006307\n",
      "Train Epoch: 8 [12160/60000]\tLoss: 0.000381\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.034854\n",
      "Train Epoch: 8 [13440/60000]\tLoss: 0.070158\n",
      "Train Epoch: 8 [14080/60000]\tLoss: 0.001416\n",
      "Train Epoch: 8 [14720/60000]\tLoss: 0.000508\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.000387\n",
      "Train Epoch: 8 [16000/60000]\tLoss: 0.001265\n",
      "Train Epoch: 8 [16640/60000]\tLoss: 0.003970\n",
      "Train Epoch: 8 [17280/60000]\tLoss: 0.008165\n",
      "Train Epoch: 8 [17920/60000]\tLoss: 0.000657\n",
      "Train Epoch: 8 [18560/60000]\tLoss: 0.003996\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 0.014826\n",
      "Train Epoch: 8 [19840/60000]\tLoss: 0.001404\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.000068\n",
      "Train Epoch: 8 [21120/60000]\tLoss: 0.021479\n",
      "Train Epoch: 8 [21760/60000]\tLoss: 0.008928\n",
      "Train Epoch: 8 [22400/60000]\tLoss: 0.001835\n",
      "Train Epoch: 8 [23040/60000]\tLoss: 0.000198\n",
      "Train Epoch: 8 [23680/60000]\tLoss: 0.001633\n",
      "Train Epoch: 8 [24320/60000]\tLoss: 0.021212\n",
      "Train Epoch: 8 [24960/60000]\tLoss: 0.005093\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.001220\n",
      "Train Epoch: 8 [26240/60000]\tLoss: 0.000362\n",
      "Train Epoch: 8 [26880/60000]\tLoss: 0.000468\n",
      "Train Epoch: 8 [27520/60000]\tLoss: 0.002016\n",
      "Train Epoch: 8 [28160/60000]\tLoss: 0.000012\n",
      "Train Epoch: 8 [28800/60000]\tLoss: 0.004302\n",
      "Train Epoch: 8 [29440/60000]\tLoss: 0.003191\n",
      "Train Epoch: 8 [30080/60000]\tLoss: 0.009034\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.000508\n",
      "Train Epoch: 8 [31360/60000]\tLoss: 0.000352\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 0.001276\n",
      "Train Epoch: 8 [32640/60000]\tLoss: 0.002834\n",
      "Train Epoch: 8 [33280/60000]\tLoss: 0.041255\n",
      "Train Epoch: 8 [33920/60000]\tLoss: 0.074417\n",
      "Train Epoch: 8 [34560/60000]\tLoss: 0.005877\n",
      "Train Epoch: 8 [35200/60000]\tLoss: 0.014230\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.047703\n",
      "Train Epoch: 8 [36480/60000]\tLoss: 0.000171\n",
      "Train Epoch: 8 [37120/60000]\tLoss: 0.004102\n",
      "Train Epoch: 8 [37760/60000]\tLoss: 0.003771\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 0.031152\n",
      "Train Epoch: 8 [39040/60000]\tLoss: 0.054065\n",
      "Train Epoch: 8 [39680/60000]\tLoss: 0.002214\n",
      "Train Epoch: 8 [40320/60000]\tLoss: 0.001170\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.007261\n",
      "Train Epoch: 8 [41600/60000]\tLoss: 0.002969\n",
      "Train Epoch: 8 [42240/60000]\tLoss: 0.000217\n",
      "Train Epoch: 8 [42880/60000]\tLoss: 0.024002\n",
      "Train Epoch: 8 [43520/60000]\tLoss: 0.006422\n",
      "Train Epoch: 8 [44160/60000]\tLoss: 0.041431\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 0.025313\n",
      "Train Epoch: 8 [45440/60000]\tLoss: 0.074488\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 0.002895\n",
      "Train Epoch: 8 [46720/60000]\tLoss: 0.000652\n",
      "Train Epoch: 8 [47360/60000]\tLoss: 0.049574\n",
      "Train Epoch: 8 [48000/60000]\tLoss: 0.000393\n",
      "Train Epoch: 8 [48640/60000]\tLoss: 0.002246\n",
      "Train Epoch: 8 [49280/60000]\tLoss: 0.003145\n",
      "Train Epoch: 8 [49920/60000]\tLoss: 0.000089\n",
      "Train Epoch: 8 [50560/60000]\tLoss: 0.008585\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.036568\n",
      "Train Epoch: 8 [51840/60000]\tLoss: 0.020479\n",
      "Train Epoch: 8 [52480/60000]\tLoss: 0.001556\n",
      "Train Epoch: 8 [53120/60000]\tLoss: 0.002637\n",
      "Train Epoch: 8 [53760/60000]\tLoss: 0.001170\n",
      "Train Epoch: 8 [54400/60000]\tLoss: 0.000110\n",
      "Train Epoch: 8 [55040/60000]\tLoss: 0.000281\n",
      "Train Epoch: 8 [55680/60000]\tLoss: 0.001388\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.176106\n",
      "Train Epoch: 8 [56960/60000]\tLoss: 0.005814\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.041613\n",
      "Train Epoch: 8 [58240/60000]\tLoss: 0.001328\n",
      "Train Epoch: 8 [58880/60000]\tLoss: 0.030387\n",
      "Train Epoch: 8 [59520/60000]\tLoss: 0.003514\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.007185\n",
      "Train Epoch: 9 [640/60000]\tLoss: 0.007723\n",
      "Train Epoch: 9 [1280/60000]\tLoss: 0.014971\n",
      "Train Epoch: 9 [1920/60000]\tLoss: 0.004126\n",
      "Train Epoch: 9 [2560/60000]\tLoss: 0.007707\n",
      "Train Epoch: 9 [3200/60000]\tLoss: 0.001452\n",
      "Train Epoch: 9 [3840/60000]\tLoss: 0.000107\n",
      "Train Epoch: 9 [4480/60000]\tLoss: 0.000558\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 0.000056\n",
      "Train Epoch: 9 [5760/60000]\tLoss: 0.002068\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 0.000719\n",
      "Train Epoch: 9 [7040/60000]\tLoss: 0.002786\n",
      "Train Epoch: 9 [7680/60000]\tLoss: 0.000346\n",
      "Train Epoch: 9 [8320/60000]\tLoss: 0.002044\n",
      "Train Epoch: 9 [8960/60000]\tLoss: 0.001059\n",
      "Train Epoch: 9 [9600/60000]\tLoss: 0.000079\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 0.000023\n",
      "Train Epoch: 9 [10880/60000]\tLoss: 0.000854\n",
      "Train Epoch: 9 [11520/60000]\tLoss: 0.002923\n",
      "Train Epoch: 9 [12160/60000]\tLoss: 0.000411\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.000262\n",
      "Train Epoch: 9 [13440/60000]\tLoss: 0.000130\n",
      "Train Epoch: 9 [14080/60000]\tLoss: 0.000927\n",
      "Train Epoch: 9 [14720/60000]\tLoss: 0.004242\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 0.014995\n",
      "Train Epoch: 9 [16000/60000]\tLoss: 0.002038\n",
      "Train Epoch: 9 [16640/60000]\tLoss: 0.005112\n",
      "Train Epoch: 9 [17280/60000]\tLoss: 0.003152\n",
      "Train Epoch: 9 [17920/60000]\tLoss: 0.010447\n",
      "Train Epoch: 9 [18560/60000]\tLoss: 0.000473\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.000272\n",
      "Train Epoch: 9 [19840/60000]\tLoss: 0.000174\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.004158\n",
      "Train Epoch: 9 [21120/60000]\tLoss: 0.005164\n",
      "Train Epoch: 9 [21760/60000]\tLoss: 0.014612\n",
      "Train Epoch: 9 [22400/60000]\tLoss: 0.022927\n",
      "Train Epoch: 9 [23040/60000]\tLoss: 0.000478\n",
      "Train Epoch: 9 [23680/60000]\tLoss: 0.000651\n",
      "Train Epoch: 9 [24320/60000]\tLoss: 0.000250\n",
      "Train Epoch: 9 [24960/60000]\tLoss: 0.002443\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.005070\n",
      "Train Epoch: 9 [26240/60000]\tLoss: 0.002135\n",
      "Train Epoch: 9 [26880/60000]\tLoss: 0.001529\n",
      "Train Epoch: 9 [27520/60000]\tLoss: 0.000090\n",
      "Train Epoch: 9 [28160/60000]\tLoss: 0.025124\n",
      "Train Epoch: 9 [28800/60000]\tLoss: 0.001560\n",
      "Train Epoch: 9 [29440/60000]\tLoss: 0.000916\n",
      "Train Epoch: 9 [30080/60000]\tLoss: 0.000253\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.007280\n",
      "Train Epoch: 9 [31360/60000]\tLoss: 0.062168\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.060863\n",
      "Train Epoch: 9 [32640/60000]\tLoss: 0.000941\n",
      "Train Epoch: 9 [33280/60000]\tLoss: 0.007218\n",
      "Train Epoch: 9 [33920/60000]\tLoss: 0.002589\n",
      "Train Epoch: 9 [34560/60000]\tLoss: 0.002296\n",
      "Train Epoch: 9 [35200/60000]\tLoss: 0.000745\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.005768\n",
      "Train Epoch: 9 [36480/60000]\tLoss: 0.001049\n",
      "Train Epoch: 9 [37120/60000]\tLoss: 0.001212\n",
      "Train Epoch: 9 [37760/60000]\tLoss: 0.004713\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.019494\n",
      "Train Epoch: 9 [39040/60000]\tLoss: 0.000037\n",
      "Train Epoch: 9 [39680/60000]\tLoss: 0.006474\n",
      "Train Epoch: 9 [40320/60000]\tLoss: 0.000991\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 0.000904\n",
      "Train Epoch: 9 [41600/60000]\tLoss: 0.001624\n",
      "Train Epoch: 9 [42240/60000]\tLoss: 0.048480\n",
      "Train Epoch: 9 [42880/60000]\tLoss: 0.005746\n",
      "Train Epoch: 9 [43520/60000]\tLoss: 0.002056\n",
      "Train Epoch: 9 [44160/60000]\tLoss: 0.001185\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.006302\n",
      "Train Epoch: 9 [45440/60000]\tLoss: 0.000837\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 0.002704\n",
      "Train Epoch: 9 [46720/60000]\tLoss: 0.003833\n",
      "Train Epoch: 9 [47360/60000]\tLoss: 0.005127\n",
      "Train Epoch: 9 [48000/60000]\tLoss: 0.072706\n",
      "Train Epoch: 9 [48640/60000]\tLoss: 0.010895\n",
      "Train Epoch: 9 [49280/60000]\tLoss: 0.025115\n",
      "Train Epoch: 9 [49920/60000]\tLoss: 0.021952\n",
      "Train Epoch: 9 [50560/60000]\tLoss: 0.003496\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.003926\n",
      "Train Epoch: 9 [51840/60000]\tLoss: 0.040958\n",
      "Train Epoch: 9 [52480/60000]\tLoss: 0.000559\n",
      "Train Epoch: 9 [53120/60000]\tLoss: 0.000024\n",
      "Train Epoch: 9 [53760/60000]\tLoss: 0.002012\n",
      "Train Epoch: 9 [54400/60000]\tLoss: 0.000503\n",
      "Train Epoch: 9 [55040/60000]\tLoss: 0.014643\n",
      "Train Epoch: 9 [55680/60000]\tLoss: 0.037304\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.005993\n",
      "Train Epoch: 9 [56960/60000]\tLoss: 0.016833\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.000519\n",
      "Train Epoch: 9 [58240/60000]\tLoss: 0.000927\n",
      "Train Epoch: 9 [58880/60000]\tLoss: 0.003222\n",
      "Train Epoch: 9 [59520/60000]\tLoss: 0.003283\n",
      "Train Epoch: 10 [0/60000]\tLoss: 0.000316\n",
      "Train Epoch: 10 [640/60000]\tLoss: 0.052861\n",
      "Train Epoch: 10 [1280/60000]\tLoss: 0.000761\n",
      "Train Epoch: 10 [1920/60000]\tLoss: 0.001938\n",
      "Train Epoch: 10 [2560/60000]\tLoss: 0.000044\n",
      "Train Epoch: 10 [3200/60000]\tLoss: 0.000927\n",
      "Train Epoch: 10 [3840/60000]\tLoss: 0.012158\n",
      "Train Epoch: 10 [4480/60000]\tLoss: 0.002663\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.000729\n",
      "Train Epoch: 10 [5760/60000]\tLoss: 0.000753\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.024639\n",
      "Train Epoch: 10 [7040/60000]\tLoss: 0.001866\n",
      "Train Epoch: 10 [7680/60000]\tLoss: 0.003036\n",
      "Train Epoch: 10 [8320/60000]\tLoss: 0.001607\n",
      "Train Epoch: 10 [8960/60000]\tLoss: 0.000296\n",
      "Train Epoch: 10 [9600/60000]\tLoss: 0.000891\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 0.013163\n",
      "Train Epoch: 10 [10880/60000]\tLoss: 0.007118\n",
      "Train Epoch: 10 [11520/60000]\tLoss: 0.000090\n",
      "Train Epoch: 10 [12160/60000]\tLoss: 0.004204\n",
      "Train Epoch: 10 [12800/60000]\tLoss: 0.000371\n",
      "Train Epoch: 10 [13440/60000]\tLoss: 0.036795\n",
      "Train Epoch: 10 [14080/60000]\tLoss: 0.000104\n",
      "Train Epoch: 10 [14720/60000]\tLoss: 0.000221\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 0.000085\n",
      "Train Epoch: 10 [16000/60000]\tLoss: 0.003071\n",
      "Train Epoch: 10 [16640/60000]\tLoss: 0.000146\n",
      "Train Epoch: 10 [17280/60000]\tLoss: 0.001883\n",
      "Train Epoch: 10 [17920/60000]\tLoss: 0.000394\n",
      "Train Epoch: 10 [18560/60000]\tLoss: 0.000578\n",
      "Train Epoch: 10 [19200/60000]\tLoss: 0.015106\n",
      "Train Epoch: 10 [19840/60000]\tLoss: 0.007914\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 0.004748\n",
      "Train Epoch: 10 [21120/60000]\tLoss: 0.000355\n",
      "Train Epoch: 10 [21760/60000]\tLoss: 0.000631\n",
      "Train Epoch: 10 [22400/60000]\tLoss: 0.013567\n",
      "Train Epoch: 10 [23040/60000]\tLoss: 0.000314\n",
      "Train Epoch: 10 [23680/60000]\tLoss: 0.011336\n",
      "Train Epoch: 10 [24320/60000]\tLoss: 0.000122\n",
      "Train Epoch: 10 [24960/60000]\tLoss: 0.003150\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 0.000594\n",
      "Train Epoch: 10 [26240/60000]\tLoss: 0.068112\n",
      "Train Epoch: 10 [26880/60000]\tLoss: 0.000093\n",
      "Train Epoch: 10 [27520/60000]\tLoss: 0.011001\n",
      "Train Epoch: 10 [28160/60000]\tLoss: 0.000929\n",
      "Train Epoch: 10 [28800/60000]\tLoss: 0.033953\n",
      "Train Epoch: 10 [29440/60000]\tLoss: 0.005033\n",
      "Train Epoch: 10 [30080/60000]\tLoss: 0.042424\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.000390\n",
      "Train Epoch: 10 [31360/60000]\tLoss: 0.000416\n",
      "Train Epoch: 10 [32000/60000]\tLoss: 0.012959\n",
      "Train Epoch: 10 [32640/60000]\tLoss: 0.001878\n",
      "Train Epoch: 10 [33280/60000]\tLoss: 0.000053\n",
      "Train Epoch: 10 [33920/60000]\tLoss: 0.000071\n",
      "Train Epoch: 10 [34560/60000]\tLoss: 0.000340\n",
      "Train Epoch: 10 [35200/60000]\tLoss: 0.000853\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.011110\n",
      "Train Epoch: 10 [36480/60000]\tLoss: 0.059905\n",
      "Train Epoch: 10 [37120/60000]\tLoss: 0.020331\n",
      "Train Epoch: 10 [37760/60000]\tLoss: 0.002171\n",
      "Train Epoch: 10 [38400/60000]\tLoss: 0.000773\n",
      "Train Epoch: 10 [39040/60000]\tLoss: 0.001782\n",
      "Train Epoch: 10 [39680/60000]\tLoss: 0.000967\n",
      "Train Epoch: 10 [40320/60000]\tLoss: 0.007178\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 0.005710\n",
      "Train Epoch: 10 [41600/60000]\tLoss: 0.001394\n",
      "Train Epoch: 10 [42240/60000]\tLoss: 0.000098\n",
      "Train Epoch: 10 [42880/60000]\tLoss: 0.004924\n",
      "Train Epoch: 10 [43520/60000]\tLoss: 0.021709\n",
      "Train Epoch: 10 [44160/60000]\tLoss: 0.037644\n",
      "Train Epoch: 10 [44800/60000]\tLoss: 0.002542\n",
      "Train Epoch: 10 [45440/60000]\tLoss: 0.004564\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.000237\n",
      "Train Epoch: 10 [46720/60000]\tLoss: 0.001824\n",
      "Train Epoch: 10 [47360/60000]\tLoss: 0.000977\n",
      "Train Epoch: 10 [48000/60000]\tLoss: 0.000089\n",
      "Train Epoch: 10 [48640/60000]\tLoss: 0.000039\n",
      "Train Epoch: 10 [49280/60000]\tLoss: 0.013642\n",
      "Train Epoch: 10 [49920/60000]\tLoss: 0.014595\n",
      "Train Epoch: 10 [50560/60000]\tLoss: 0.024594\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.011512\n",
      "Train Epoch: 10 [51840/60000]\tLoss: 0.000216\n",
      "Train Epoch: 10 [52480/60000]\tLoss: 0.000298\n",
      "Train Epoch: 10 [53120/60000]\tLoss: 0.000383\n",
      "Train Epoch: 10 [53760/60000]\tLoss: 0.000438\n",
      "Train Epoch: 10 [54400/60000]\tLoss: 0.000833\n",
      "Train Epoch: 10 [55040/60000]\tLoss: 0.002034\n",
      "Train Epoch: 10 [55680/60000]\tLoss: 0.000330\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.000284\n",
      "Train Epoch: 10 [56960/60000]\tLoss: 0.003602\n",
      "Train Epoch: 10 [57600/60000]\tLoss: 0.000416\n",
      "Train Epoch: 10 [58240/60000]\tLoss: 0.000033\n",
      "Train Epoch: 10 [58880/60000]\tLoss: 0.001927\n",
      "Train Epoch: 10 [59520/60000]\tLoss: 0.003912\n",
      "\n",
      "Test set: Avg. loss: 0.0339, Accuracy: 9915/10000 (99.15%)\n",
      "\n",
      "Model 3 Accuracy: 98.79%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Dropout layer to reduce overfitting\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        # Second convolutional block\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        # Third convolutional block\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        # Fully connected layers with ReLU and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        # Output layer with log softmax\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model, optimizer, and other components\n",
    "model3 = Net3()\n",
    "optimizer = optim.Adam(model3.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Train Epoch: {epoch+1} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Testing function\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Avg. loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    return accuracy\n",
    "\n",
    "# Train and test Net3\n",
    "train_model(model3, epochs=10)\n",
    "accuracy3 = test_model(model3)\n",
    "\n",
    "\n",
    "print(f\"Model 3 Accuracy: {round(float(accuracy2.numpy()),2)}%\")\n",
    "\n",
    "# Run network on data we got before and show predictions\n",
    "output = model3(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ7a-wA6oPJj"
   },
   "source": [
    "# Optimized Model 3 Observation:\n",
    "\n",
    "**Model 3 (Net3): Improved Convolutional Neural Network for MNIST Classification**\n",
    "\n",
    "Model 3 is designed with additional complexity and regularization techniques to achieve high accuracy on the MNIST dataset. Different from Models 1 and 2 which use fewer convolutional layers and feature maps, Model 3 includes three convolutional layers with 32, 64 and 128 feature maps respectively, each convoluti#onal layer is followed by batch normalization and Max pooling. This depth enables the model to capture the intricate details and layered patterns in the image data, which are critical for distinguishing similar figures.\n",
    "\n",
    "**Performance**\n",
    "\n",
    "Adding batch normalization and dropout to Model 3 makes it more resistant to overfitting and improves its generalization ability. The Adam optimizer can dynamically adjust the learning rate to further accelerate convergence. Therefore, Model 3 achieves a lower average loss of 0.0252 and a higher accuracy of 99.27% ​​on the test set, which is better than Models 1 and 2.\n",
    "\n",
    "**Compromise: Increased computation time**\n",
    "\n",
    "Although Model 3 provides greater accuracy, it requires more running time due to its increased complexity:\n",
    "- Increasing the number of layers and feature maps: significantly increases the computational load because the operations of each layer scale with the number of parameters.\n",
    "- Batch Normalization and Abort: introduces additional operations that further increase processing time.\n",
    "- Model 3 takes 4 minutes to run on my local machine, while models 1 and 2 take 29 seconds and 2 minutes respectively.\n",
    "\n",
    "In summary, Model 3's advanced architecture and regularization strategy provide higher accuracy at the cost of longer execution time, illustrating the trade-off between performance and computational efficiency."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
