{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZlU8khsZjZT"
      },
      "source": [
        "# **Week 4: Colab Experiment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3LRo3ehZo2B"
      },
      "source": [
        "# I. Introduction\n",
        "In this exercise, we load the Breast cancer wisconsin dataset for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2Bcr9sZofG"
      },
      "source": [
        "# II. Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code loads the breast cancer wisconsin dataset, and classifies between benign and malignant.\n",
        "\n",
        "The dataset was split into five parts using K-Fold cross-validation. From this, training and test sets were generated, and three machine learning models—Logistic Regression, SVM, and Decision Tree—were trained and evaluated. Finally, the mean error and standard deviation of the error rates were calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "X4dRDQZqqiet"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import zero_one_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ArV6oId2qjCh"
      },
      "outputs": [],
      "source": [
        "# Define the dependent and independent variables.\n",
        "data = load_breast_cancer()\n",
        "Y = data.target\n",
        "X = data.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_kY6lUBXL0TX"
      },
      "outputs": [],
      "source": [
        "# Create a KFold object that will split data into 5 folds for\n",
        "# cross-validation, shuffling the data\n",
        "# and using a random seed (7) for reproducibility.\n",
        "num_folds = 5\n",
        "kf = KFold(n_splits=num_folds, random_state=0, shuffle=True)\n",
        "\n",
        "# Initialize a dictionary to store the indices of the training and test sets\n",
        "kfold_indices = {}\n",
        "\n",
        "# Loop over the 5 folds and store the indices in the dictionary\n",
        "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "  kfold_indices[f\"fold_{i}\"] = {'train': train_index, 'test': test_index}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "UsTfhZNxL0V1"
      },
      "outputs": [],
      "source": [
        "# Train models and apply them to the test set\n",
        "\n",
        "# Initialize a dictionary to store the error rates of the models\n",
        "Error_rate = {'log_reg': [], 'svm': [], 'decision_tree': []}\n",
        "\n",
        "# Loop over the 5 folds\n",
        "for fold_id in range(num_folds):\n",
        "\n",
        "  # Training data extraction \n",
        "  X_train = X[kfold_indices[f\"fold_{fold_id}\"]['train']]\n",
        "  Y_train = Y[kfold_indices[f\"fold_{fold_id}\"]['train']]\n",
        "  # Test data extraction\n",
        "  X_test = X[kfold_indices[f\"fold_{fold_id}\"]['test']]\n",
        "  Y_test = Y[kfold_indices[f\"fold_{fold_id}\"]['test']]\n",
        "\n",
        "  # Logistic regression\n",
        "  # Create a pipeline that removes the mean and scale\n",
        "  # to unit variance, then applies logistic regression \n",
        "  # with a max of 10,000 iterations\n",
        "  log_reg_pipeline = Pipeline([\n",
        "      ('scaler', StandardScaler()),\n",
        "      ('log_reg', LogisticRegression(max_iter=10000))\n",
        "  ])\n",
        "  # Fit the pipeline to the training data\n",
        "  log_reg_pipeline.fit(X_train, Y_train)\n",
        "  # Predict the test data\n",
        "  Y_pred_log_reg = log_reg_pipeline.predict(X_test)\n",
        "  # Calculate the error\n",
        "  error_log_reg = zero_one_loss(Y_test, Y_pred_log_reg)\n",
        "  # Append the error to the dictionary\n",
        "  Error_rate['log_reg'].append(error_log_reg)\n",
        "\n",
        "  # SVM\n",
        "  # Create a pipeline that removes the mean and scale\n",
        "  # to unit variance, then applies a support vector machine\n",
        "  svm_pipeline = Pipeline([\n",
        "      ('scaler', StandardScaler()),\n",
        "      ('svm', SVC())\n",
        "  ])\n",
        "  # Fit the pipeline to the training data\n",
        "  svm_pipeline.fit(X_train, Y_train)\n",
        "  # Predict the test data\n",
        "  Y_pred_svm = svm_pipeline.predict(X_test)\n",
        "  # Calculate the error\n",
        "  error_svm = zero_one_loss(Y_test, Y_pred_svm)\n",
        "  # Append the error to the dictionary\n",
        "  Error_rate['svm'].append(error_svm)\n",
        "\n",
        "  # Decision tree\n",
        "  # Create a pipeline that removes the mean and scale\n",
        "  # to unit variance, then applies a decision tree\n",
        "  decision_tree_pipeline = Pipeline([\n",
        "      ('scaler', StandardScaler()),\n",
        "      ('dt', DecisionTreeClassifier())\n",
        "  ])\n",
        "  # Fit the pipeline to the training data\n",
        "  decision_tree_pipeline.fit(X_train, Y_train)\n",
        "  # Predict the test data\n",
        "  Y_pred_dt = decision_tree_pipeline.predict(X_test)\n",
        "  # Calculate the error\n",
        "  error_dt = zero_one_loss(Y_test, Y_pred_dt)\n",
        "  # Append the error to the dictionary\n",
        "  Error_rate['decision_tree'].append(error_dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW0uMLYwZ63z"
      },
      "source": [
        "## III. Results\n",
        "\n",
        "Here we report the mean and standard deviation of the error rates over 5 folds for each method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uD1iPyJP25T",
        "outputId": "743d194a-5649-40de-d0cd-c7120933da4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The error rate over 5 folds in CV:\n",
            "log_reg: mean = 0.0281, std = 0.0170\n",
            "svm: mean = 0.0211, std = 0.0131\n",
            "decision_tree: mean = 0.0773, std = 0.0237\n"
          ]
        }
      ],
      "source": [
        "print(f\"The error rate over 5 folds in CV:\")\n",
        "\n",
        "# Calculate the mean and standard deviation of the error rates\n",
        "for method, errors in Error_rate.items():\n",
        "    mean_error = np.mean(errors)\n",
        "    std_error = np.std(errors)\n",
        "    print(f\"{method}: mean = {mean_error:.4f}, std = {std_error:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srwwVH9TaBm3"
      },
      "source": [
        "# IV. Conclusion and Discussion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, the results are:\n",
        "1. SVM performs the best overall, with the lowest mean error and the smallest standard deviation. This suggests that it is the most accurate and the most stable model for this dataset.\n",
        "2. Logistic Regression performs well, with a slightly higher mean error and standard deviation. This indicates that it is still a very good option for this classification problem.\n",
        "3. The Decision Tree performs the worst, with the highest mean error and standard deviation. This shows that the Decision Tree is less accurate and its performance fluctuates more between different folds. The reason to this might be the following.\n",
        "\n",
        "    (i) Overfitting: Decision Trees are likely to encounter overfitting, especially if not pruned or regularized.\n",
        "    \n",
        "    (ii) Complexity: Decision Trees are not linear models, whereas logistic regression and SVM are. On a dataset like the breast cancer dataset, which might not have complex decision boundaries, the simplicity of linear models might be more suitable, leading to better performance.\n",
        "\n",
        "Next, I am going to some other classification methods taught in the lecture: Bagging and Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The error rate over 5 folds in CV:\n",
            "bagging: mean = 0.0386, std = 0.0172\n",
            "random_forest: mean = 0.0386, std = 0.0131\n"
          ]
        }
      ],
      "source": [
        "# Required Libraries\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "Y = data.target\n",
        "\n",
        "# Create a KFold object for 5-fold cross-validation \n",
        "num_folds = 5\n",
        "kf = KFold(n_splits=num_folds, random_state=0, shuffle=True)\n",
        "\n",
        "# Initialize a dictionary to store the error rates of the models\n",
        "Error_rate = {'bagging': [], 'random_forest': []}\n",
        "\n",
        "# Loop over the 5 folds\n",
        "for fold_id, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "    # Training and Test Data extractions\n",
        "    X_train = X[train_index]\n",
        "    Y_train = Y[train_index]\n",
        "    X_test = X[test_index]\n",
        "    Y_test = Y[test_index]\n",
        "\n",
        "    # Bagging\n",
        "    # Create a pipeline to scale the data and apply the Bagging classifier\n",
        "    bagging_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('bagging', BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                                      n_estimators=100, max_samples=1.0, \n",
        "                                      random_state=0, n_jobs=-1)) \n",
        "    ])\n",
        "    # Fit the pipeline to the training data\n",
        "    bagging_pipeline.fit(X_train, Y_train)\n",
        "    # Predict the test data\n",
        "    Y_pred_bagging = bagging_pipeline.predict(X_test)\n",
        "    # Calculate the error\n",
        "    error_bagging = zero_one_loss(Y_test, Y_pred_bagging)\n",
        "    # Append the error to the dictionary\n",
        "    Error_rate['bagging'].append(error_bagging)\n",
        "\n",
        "    # Random Forest\n",
        "    # Create a pipeline to scale the data and apply the Random Forest classifier\n",
        "    random_forest_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('random_forest', RandomForestClassifier(n_estimators=100, max_depth=None,\n",
        "                                                 random_state=0, n_jobs=-1, \n",
        "                                                 max_features='sqrt')) \n",
        "    ])\n",
        "    # Fit the pipeline to the training data\n",
        "    random_forest_pipeline.fit(X_train, Y_train)\n",
        "    # Predict the test data\n",
        "    Y_pred_rf = random_forest_pipeline.predict(X_test)\n",
        "    # Calculate the error\n",
        "    error_rf = zero_one_loss(Y_test, Y_pred_rf)\n",
        "    # Append the error to the dictionary\n",
        "    Error_rate['random_forest'].append(error_rf)\n",
        "\n",
        "# Print the results\n",
        "print(f\"The error rate over 5 folds in CV:\")\n",
        "\n",
        "# Calculate the mean and standard deviation of the error rates\n",
        "for method, errors in Error_rate.items():\n",
        "    mean_error = np.mean(errors)\n",
        "    std_error = np.std(errors)\n",
        "    print(f\"{method}: mean = {mean_error:.4f}, std = {std_error:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall, Bagging and Random Forest outperform Decision Trees, yet they do not surpass the performance of Logistic Regression and SVM. This suggests that Bagging and Random Forest effectively mitigate the issue of overfitting and provide more stable predictions. However, the increased model complexity introduced by these ensemble methods do not result in better outcomes than those achieved with Logistic Regression and SVM for this particular dataset. Therefore, it may be useful to explore other simpler training models or consider advanced hyperparameter optimization techniques."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
